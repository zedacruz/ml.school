[
  {
    "objectID": "cohort.html",
    "href": "cohort.html",
    "title": "Building Machine Learning Systems That Don’t Suck",
    "section": "",
    "text": "This notebook creates a SageMaker Pipeline to build an end-to-end Machine Learning system to solve the problem of classifying penguin species. With a SageMaker Pipeline, you can create, automate, and manage end-to-end Machine Learning workflows at scale.\nYou can find more information about Amazon SageMaker in the Amazon SageMaker Developer Guide. The AWS Machine Learning Blog is an excellent source to stay up-to-date with SageMaker.\nThis example uses the Penguins dataset.\nThis notebook is part of the Machine Learning School program.",
    "crumbs": [
      "Building Machine Learning Systems That Don't Suck"
    ]
  },
  {
    "objectID": "cohort.html#session-1---introduction-and-initial-setup",
    "href": "cohort.html#session-1---introduction-and-initial-setup",
    "title": "Building Machine Learning Systems That Don’t Suck",
    "section": "Session 1 - Introduction and Initial Setup",
    "text": "Session 1 - Introduction and Initial Setup\nThe machine learning system we’ll build during this program consists of four main pipelines: A training pipeline, an inference pipeline, a deployment pipeline, and a monitoring pipeline.\nHere is an architectural diagram showing how the system is structured:\n \nThroughout the sessions, we’ll build each of these pipelines. We’ll also build variations to show you different alternatives and best practices.\nLet’s start by setting up the environment and preparing to run the notebook.\nWe can run this notebook in Local Mode to test some of the system components in your local environment. Unfortunately, not every component is supported in Local Mode.\nSetting the LOCAL_MODE variable to True will run every supported pipeline component locally. Setting the variable to False will run the pipeline in SageMaker.\n\nLOCAL_MODE = True\n\nLet’s now load the environment variables we need to run the notebook.\n\nimport os\n\nbucket = os.environ[\"BUCKET\"]\nrole = os.environ[\"ROLE\"]\n\nCOMET_API_KEY = os.environ.get(\"COMET_API_KEY\", None)\nCOMET_PROJECT_NAME = os.environ.get(\"COMET_PROJECT_NAME\", None)\n\nIf you are running the pipeline in Local Mode on an ARM64 machine (for example, on Apple Silicon), you will need to use a custom Docker image to train and evaluate the model. Let’s create a variable indicating if we are running on an ARM64 machine.\n\n# We can retrieve the architecture of the local\n# computer using the `uname -m` command.\narchitecture = !(uname -m)\n\nIS_ARM64_ARCHITECTURE = architecture[0] == \"arm64\"\n\nLet’s create a configuration dictionary with different settings depending on whether we are running the pipeline in Local Mode. We’ll use this dictionary to configure the pipeline components.\n\nimport sagemaker\nfrom sagemaker.workflow.pipeline_context import LocalPipelineSession, PipelineSession\n\npipeline_session = PipelineSession(default_bucket=bucket) if not LOCAL_MODE else None\n\nif LOCAL_MODE:\n    config = {\n        \"session\": LocalPipelineSession(default_bucket=bucket),\n        \"instance_type\": \"local\",\n        # We need to use a custom Docker image when we run the pipeline\n        # in Local Model on an ARM64 machine.\n        \"image\": (\n            \"sagemaker-tensorflow-toolkit-local\" if IS_ARM64_ARCHITECTURE else None\n        ),\n    }\nelse:\n    config = {\n        \"session\": pipeline_session,\n        \"instance_type\": \"ml.m5.xlarge\",\n        \"image\": None,\n    }\n\n# These specific settings refer to the SageMaker\n# TensorFlow container we'll use.\nconfig[\"framework_version\"] = \"2.12\"\nconfig[\"py_version\"] = \"py310\"\n\nLet’s now initialize a few variables that we’ll need throughout the notebook:\n\nimport boto3\n\nS3_LOCATION = f\"s3://{bucket}/penguins\"\n\nsagemaker_session = sagemaker.session.Session()\nsagemaker_client = boto3.client(\"sagemaker\")\niam_client = boto3.client(\"iam\")\nregion = boto3.Session().region_name",
    "crumbs": [
      "Building Machine Learning Systems That Don't Suck"
    ]
  },
  {
    "objectID": "cohort.html#session-2---exploratory-data-analysis",
    "href": "cohort.html#session-2---exploratory-data-analysis",
    "title": "Building Machine Learning Systems That Don’t Suck",
    "section": "Session 2 - Exploratory Data Analysis",
    "text": "Session 2 - Exploratory Data Analysis\nLet’s run Exploratory Data Analysis on the Penguins dataset. The goal of this session is to understand the data and the problem we are trying to solve.\nLet’s load the Penguins dataset:\n\nimport numpy as np\nimport pandas as pd\n\npenguins = pd.read_csv(DATA_FILEPATH)\npenguins.head()\n\n\n\n\n\n\n\n\n\nspecies\nisland\nculmen_length_mm\nculmen_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nMALE\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n\n\n\n\n\n\n\n\nWe can see the dataset contains the following columns:\n\nspecies: The species of a penguin. This is the column we want to predict.\nisland: The island where the penguin was found\nculmen_length_mm: The length of the penguin’s culmen (bill) in millimeters\nculmen_depth_mm: The depth of the penguin’s culmen in millimeters\nflipper_length_mm: The length of the penguin’s flipper in millimeters\nbody_mass_g: The body mass of the penguin in grams\nsex: The sex of the penguin\n\nIf you are curious, here is the description of a penguin’s culmen:\n\nNow, let’s get the summary statistics for the features in our dataset.\n\npenguins.describe(include=\"all\")\n\n\n\n\n\n\n\n\n\nspecies\nisland\nculmen_length_mm\nculmen_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\ncount\n344\n344\n342.000000\n342.000000\n342.000000\n342.000000\n334\n\n\nunique\n3\n3\nNaN\nNaN\nNaN\nNaN\n3\n\n\ntop\nAdelie\nBiscoe\nNaN\nNaN\nNaN\nNaN\nMALE\n\n\nfreq\n152\n168\nNaN\nNaN\nNaN\nNaN\n168\n\n\nmean\nNaN\nNaN\n43.921930\n17.151170\n200.915205\n4201.754386\nNaN\n\n\nstd\nNaN\nNaN\n5.459584\n1.974793\n14.061714\n801.954536\nNaN\n\n\nmin\nNaN\nNaN\n32.100000\n13.100000\n172.000000\n2700.000000\nNaN\n\n\n25%\nNaN\nNaN\n39.225000\n15.600000\n190.000000\n3550.000000\nNaN\n\n\n50%\nNaN\nNaN\n44.450000\n17.300000\n197.000000\n4050.000000\nNaN\n\n\n75%\nNaN\nNaN\n48.500000\n18.700000\n213.000000\n4750.000000\nNaN\n\n\nmax\nNaN\nNaN\n59.600000\n21.500000\n231.000000\n6300.000000\nNaN\n\n\n\n\n\n\n\n\nLet’s now display the distribution of values for the three categorical columns in our data:\n\nspecies_distribution = penguins[\"species\"].value_counts()\nisland_distribution = penguins[\"island\"].value_counts()\nsex_distribution = penguins[\"sex\"].value_counts()\n\nprint(species_distribution, end=\"\\n\\n\")\nprint(island_distribution, end=\"\\n\\n\")\nprint(sex_distribution)\n\nspecies\nAdelie       152\nGentoo       124\nChinstrap     68\nName: count, dtype: int64\n\nisland\nBiscoe       168\nDream        124\nTorgersen     52\nName: count, dtype: int64\n\nsex\nMALE      168\nFEMALE    165\n.           1\nName: count, dtype: int64\n\n\nThe distribution of the categories in our data are:\n\nspecies: There are 3 species of penguins in the dataset: Adelie (152), Gentoo (124), and Chinstrap (68).\nisland: Penguins are from 3 islands: Biscoe (168), Dream (124), and Torgersen (52).\nsex: We have 168 male penguins, 165 female penguins, and 1 penguin with an ambiguous gender (.).\n\nLet’s replace the ambiguous value in the sex column with a null value:\n\npenguins[\"sex\"] = penguins[\"sex\"].replace(\".\", np.nan)\n\n# Let's display the new distribution of the column:\nsex_distribution = penguins[\"sex\"].value_counts()\nsex_distribution\n\nsex\nMALE      168\nFEMALE    165\nName: count, dtype: int64\n\n\nNext, let’s check for any missing values in the dataset.\n\npenguins.isna().sum()\n\nspecies               0\nisland                0\nculmen_length_mm      2\nculmen_depth_mm       2\nflipper_length_mm     2\nbody_mass_g           2\nsex                  11\ndtype: int64\n\n\nLet’s get rid of the missing values. For now, we are going to replace the missing values with the most frequent value in the column. Later, we’ll use a different strategy to replace missing numeric values.\n\nfrom sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(strategy=\"most_frequent\")\npenguins.iloc[:, :] = imputer.fit_transform(penguins)\n\n# Let's display again the number of missing values:\npenguins.isna().sum()\n\nspecies              0\nisland               0\nculmen_length_mm     0\nculmen_depth_mm      0\nflipper_length_mm    0\nbody_mass_g          0\nsex                  0\ndtype: int64\n\n\nLet’s visualize the distribution of categorical features.\n\nimport matplotlib.pyplot as plt\n\nfig, axs = plt.subplots(3, 1, figsize=(6, 10))\n\naxs[0].bar(species_distribution.index, species_distribution.values)\naxs[0].set_ylabel(\"Count\")\naxs[0].set_title(\"Distribution of Species\")\n\naxs[1].bar(island_distribution.index, island_distribution.values)\naxs[1].set_ylabel(\"Count\")\naxs[1].set_title(\"Distribution of Island\")\n\naxs[2].bar(sex_distribution.index, sex_distribution.values)\naxs[2].set_ylabel(\"Count\")\naxs[2].set_title(\"Distribution of Sex\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nLet’s visualize the distribution of numerical columns.\n\nfig, axs = plt.subplots(2, 2, figsize=(8, 6))\n\naxs[0, 0].hist(penguins[\"culmen_length_mm\"], bins=20)\naxs[0, 0].set_ylabel(\"Count\")\naxs[0, 0].set_title(\"Distribution of culmen_length_mm\")\n\naxs[0, 1].hist(penguins[\"culmen_depth_mm\"], bins=20)\naxs[0, 1].set_ylabel(\"Count\")\naxs[0, 1].set_title(\"Distribution of culmen_depth_mm\")\n\naxs[1, 0].hist(penguins[\"flipper_length_mm\"], bins=20)\naxs[1, 0].set_ylabel(\"Count\")\naxs[1, 0].set_title(\"Distribution of flipper_length_mm\")\n\naxs[1, 1].hist(penguins[\"body_mass_g\"], bins=20)\naxs[1, 1].set_ylabel(\"Count\")\naxs[1, 1].set_title(\"Distribution of body_mass_g\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nLet’s display the covariance matrix of the dataset. The “covariance” measures how changes in one variable are associated with changes in a second variable. In other words, the covariance measures the degree to which two variables are linearly associated.\n\npenguins.cov(numeric_only=True)\n\n\n\n\n\n\n\n\n\nculmen_length_mm\nculmen_depth_mm\nflipper_length_mm\nbody_mass_g\n\n\n\n\nculmen_length_mm\n29.679415\n-2.516984\n50.260588\n2596.971151\n\n\nculmen_depth_mm\n-2.516984\n3.877201\n-16.108849\n-742.660180\n\n\nflipper_length_mm\n50.260588\n-16.108849\n197.269501\n9792.552037\n\n\nbody_mass_g\n2596.971151\n-742.660180\n9792.552037\n640316.716388\n\n\n\n\n\n\n\n\nHere are three examples of what we get from interpreting the covariance matrix below:\n\nThe positive covariance of 50.26 between culmen length and flippler length suggests that larger values of culmen length are associated with larger values of flipper length. As one increases, generally so does the other.\nThe positive covariance of 2596.97 between culmen length and body mass suggests that heavier penguins generally have longer culmens. There is a tendency for these two variables to increase together.\nThe negative covariance of -742.66 between culmen depth and body mass suggests a general tendency that penguins with deeper culmens weigh less.\n\nLet’s now display the correlation matrix. “Correlation” measures both the strength and direction of the linear relationship between two variables:\n\npenguins.corr(numeric_only=True)\n\n\n\n\n\n\n\n\n\nculmen_length_mm\nculmen_depth_mm\nflipper_length_mm\nbody_mass_g\n\n\n\n\nculmen_length_mm\n1.000000\n-0.234635\n0.656856\n0.595720\n\n\nculmen_depth_mm\n-0.234635\n1.000000\n-0.582472\n-0.471339\n\n\nflipper_length_mm\n0.656856\n-0.582472\n1.000000\n0.871302\n\n\nbody_mass_g\n0.595720\n-0.471339\n0.871302\n1.000000\n\n\n\n\n\n\n\n\nHere are three examples of what we get from interpreting the correlation matrix below:\n\nPenguins that weight more tend to have longer flippers.\nPenguins with a shallower culmen tend to have longer flippers.\nPenguins with longer culmens tend to have longer flippers.\n\nLet’s display the distribution of species by island:\n\nunique_species = penguins[\"species\"].unique()\n\nfig, ax = plt.subplots(figsize=(6, 6))\nfor species in unique_species:\n    data = penguins[penguins[\"species\"] == species]\n    ax.hist(data[\"island\"], bins=5, alpha=0.5, label=species)\n\nax.set_xlabel(\"Island\")\nax.set_ylabel(\"Count\")\nax.set_title(\"Distribution of Species by Island\")\nax.legend()\nplt.show()\n\n\n\n\n\n\n\n\nLet’s display the distribution of species by sex.\n\nfig, ax = plt.subplots(figsize=(6, 6))\n\nfor species in unique_species:\n    data = penguins[penguins[\"species\"] == species]\n    ax.hist(data[\"sex\"], bins=3, alpha=0.5, label=species)\n\nax.set_xlabel(\"Sex\")\nax.set_ylabel(\"Count\")\nax.set_title(\"Distribution of Species by Sex\")\n\nax.legend()\nplt.show()",
    "crumbs": [
      "Building Machine Learning Systems That Don't Suck"
    ]
  },
  {
    "objectID": "cohort.html#session-3---splitting-and-transforming-the-data",
    "href": "cohort.html#session-3---splitting-and-transforming-the-data",
    "title": "Building Machine Learning Systems That Don’t Suck",
    "section": "Session 3 - Splitting and Transforming the Data",
    "text": "Session 3 - Splitting and Transforming the Data\nIn this session we’ll build a simple SageMaker Pipeline with one step to split and transform the data:\n \nWe’ll use a Scikit-Learn Pipeline for the transformations, and a Processing Step with a SKLearnProcessor to execute a preprocessing script. Check the SageMaker Pipelines Overview for an introduction to the fundamental components of a SageMaker Pipeline.\n\nStep 1 - Creating the Preprocessing Script\nThe first step we need in the pipeline is a Processing Step to run a script that will split and transform the data.\nThis Processing Step will create a SageMaker Processing Job in the background, run the script, and upload the output to S3. You can use Processing Jobs to perform data preprocessing, post-processing, feature engineering, data validation, and model evaluation. Check the ProcessingStep SageMaker’s SDK documentation for more information.\nWe will store the script in a folder called processing and add it to the system path so we can later import it as a module.\n\n(CODE_FOLDER / \"processing\").mkdir(parents=True, exist_ok=True)\nsys.path.extend([f\"./{CODE_FOLDER}/processing\"])\n\nLet’s now create the script:\n\n\n\nscript.py\n\nimport os\nimport tarfile\nimport tempfile\nfrom pathlib import Path\n\nimport joblib\nimport numpy as np\nimport pandas as pd\nfrom sklearn.compose import ColumnTransformer, make_column_selector\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n\n\ndef preprocess(base_directory):\n    \"\"\"Load the supplied data, split it and transform it.\"\"\"\n    df = _read_data_from_input_csv_files(base_directory)\n\n    target_transformer = ColumnTransformer(\n        transformers=[(\"species\", OrdinalEncoder(), [0])],\n    )\n\n    numeric_transformer = make_pipeline(\n        SimpleImputer(strategy=\"mean\"),\n        StandardScaler(),\n    )\n\n    categorical_transformer = make_pipeline(\n        SimpleImputer(strategy=\"most_frequent\"),\n        OneHotEncoder(),\n    )\n\n    features_transformer = ColumnTransformer(\n        transformers=[\n            (\n                \"numeric\",\n                numeric_transformer,\n                make_column_selector(dtype_exclude=\"object\"),\n            ),\n            (\"categorical\", categorical_transformer, [\"island\"]),\n        ],\n    )\n\n    df_train, df_validation, df_test = _split_data(df)\n\n    _save_train_baseline(base_directory, df_train)\n    _save_test_baseline(base_directory, df_test)\n\n    y_train = target_transformer.fit_transform(\n        np.array(df_train.species.values).reshape(-1, 1),\n    )\n    y_validation = target_transformer.transform(\n        np.array(df_validation.species.values).reshape(-1, 1),\n    )\n    y_test = target_transformer.transform(\n        np.array(df_test.species.values).reshape(-1, 1),\n    )\n\n    df_train = df_train.drop(\"species\", axis=1)\n    df_validation = df_validation.drop(\"species\", axis=1)\n    df_test = df_test.drop(\"species\", axis=1)\n\n    X_train = features_transformer.fit_transform(df_train)  # noqa: N806\n    X_validation = features_transformer.transform(df_validation)  # noqa: N806\n    X_test = features_transformer.transform(df_test)  # noqa: N806\n\n    _save_splits(\n        base_directory,\n        X_train,\n        y_train,\n        X_validation,\n        y_validation,\n        X_test,\n        y_test,\n    )\n    _save_model(base_directory, target_transformer, features_transformer)\n\n\ndef _read_data_from_input_csv_files(base_directory):\n    \"\"\"Read the data from the input CSV files.\n\n    This function reads every CSV file available and\n    concatenates them into a single dataframe.\n    \"\"\"\n    input_directory = Path(base_directory) / \"input\"\n    files = list(input_directory.glob(\"*.csv\"))\n\n    if len(files) == 0:\n        message = f\"The are no CSV files in {input_directory.as_posix()}/\"\n        raise ValueError(message)\n\n    raw_data = [pd.read_csv(file) for file in files]\n    df = pd.concat(raw_data)\n\n    # Shuffle the data\n    return df.sample(frac=1, random_state=42)\n\n\ndef _split_data(df):\n    \"\"\"Split the data into train, validation, and test.\"\"\"\n    df_train, temp = train_test_split(df, test_size=0.3)\n    df_validation, df_test = train_test_split(temp, test_size=0.5)\n\n    return df_train, df_validation, df_test\n\n\ndef _save_train_baseline(base_directory, df_train):\n    \"\"\"Save the untransformed training data to disk.\n\n    We will need the training data to compute a baseline to\n    determine the quality of the data that the model receives\n    when deployed.\n    \"\"\"\n    baseline_path = Path(base_directory) / \"train-baseline\"\n    baseline_path.mkdir(parents=True, exist_ok=True)\n\n    df = df_train.copy().dropna()\n\n    # To compute the data quality baseline, we don't need the\n    # target variable, so we'll drop it from the dataframe.\n    df = df.drop(\"species\", axis=1)\n\n    df.to_csv(baseline_path / \"train-baseline.csv\", header=True, index=False)\n\n\ndef _save_test_baseline(base_directory, df_test):\n    \"\"\"Save the untransformed test data to disk.\n\n    We will need the test data to compute a baseline to\n    determine the quality of the model predictions when deployed.\n    \"\"\"\n    baseline_path = Path(base_directory) / \"test-baseline\"\n    baseline_path.mkdir(parents=True, exist_ok=True)\n\n    df = df_test.copy().dropna()\n\n    # We'll use the test baseline to generate predictions later,\n    # and we can't have a header line because the model won't be\n    # able to make a prediction for it.\n    df.to_csv(baseline_path / \"test-baseline.csv\", header=False, index=False)\n\n\ndef _save_splits(\n    base_directory,\n    X_train,  # noqa: N803\n    y_train,\n    X_validation,  # noqa: N803\n    y_validation,\n    X_test,  # noqa: N803\n    y_test,\n):\n    \"\"\"Save data splits to disk.\n\n    This function concatenates the transformed features\n    and the target variable, and saves each one of the split\n    sets to disk.\n    \"\"\"\n    train = np.concatenate((X_train, y_train), axis=1)\n    validation = np.concatenate((X_validation, y_validation), axis=1)\n    test = np.concatenate((X_test, y_test), axis=1)\n\n    train_path = Path(base_directory) / \"train\"\n    validation_path = Path(base_directory) / \"validation\"\n    test_path = Path(base_directory) / \"test\"\n\n    train_path.mkdir(parents=True, exist_ok=True)\n    validation_path.mkdir(parents=True, exist_ok=True)\n    test_path.mkdir(parents=True, exist_ok=True)\n\n    pd.DataFrame(train).to_csv(train_path / \"train.csv\", header=False, index=False)\n    pd.DataFrame(validation).to_csv(\n        validation_path / \"validation.csv\",\n        header=False,\n        index=False,\n    )\n    pd.DataFrame(test).to_csv(test_path / \"test.csv\", header=False, index=False)\n\n\ndef _save_model(base_directory, target_transformer, features_transformer):\n    \"\"\"Save the Scikit-Learn transformation pipelines.\n\n    This function creates a model.tar.gz file that\n    contains the two transformation pipelines we built\n    to transform the data.\n    \"\"\"\n    with tempfile.TemporaryDirectory() as directory:\n        joblib.dump(target_transformer, Path(directory) / \"target.joblib\")\n        joblib.dump(features_transformer, Path(directory) / \"features.joblib\")\n\n        model_path = Path(base_directory) / \"model\"\n        model_path.mkdir(parents=True, exist_ok=True)\n\n        with tarfile.open(f\"{(model_path / 'model.tar.gz').as_posix()}\", \"w:gz\") as tar:\n            tar.add(Path(directory) / \"target.joblib\", arcname=\"target.joblib\")\n            tar.add(\n                Path(directory) / \"features.joblib\", arcname=\"features.joblib\",\n            )\n\n\nif __name__ == \"__main__\":\n    preprocess(base_directory=\"/opt/ml/processing\")\n\n\nLet’s test the script to ensure everything is working as expected:\n\n\nCode\nimport os\nimport shutil\nimport tarfile\nimport tempfile\n\nimport pytest\nfrom processing.script import preprocess\n\n\n@pytest.fixture(autouse=False)\ndef directory():\n    directory = tempfile.mkdtemp()\n    input_directory = Path(directory) / \"input\"\n    input_directory.mkdir(parents=True, exist_ok=True)\n    shutil.copy2(DATA_FILEPATH, input_directory / \"data.csv\")\n\n    directory = Path(directory)\n    preprocess(base_directory=directory)\n\n    yield directory\n\n    shutil.rmtree(directory)\n\n\ndef test_preprocess_generates_data_splits(directory):\n    output_directories = os.listdir(directory)\n\n    assert \"train\" in output_directories\n    assert \"validation\" in output_directories\n    assert \"test\" in output_directories\n\n\ndef test_preprocess_generates_baselines(directory):\n    output_directories = os.listdir(directory)\n\n    assert \"train-baseline\" in output_directories\n    assert \"test-baseline\" in output_directories\n\n\ndef test_preprocess_creates_two_models(directory):\n    model_path = directory / \"model\"\n    tar = tarfile.open(model_path / \"model.tar.gz\", \"r:gz\")\n\n    assert \"features.joblib\" in tar.getnames()\n    assert \"target.joblib\" in tar.getnames()\n\n\ndef test_splits_are_transformed(directory):\n    train = pd.read_csv(directory / \"train\" / \"train.csv\", header=None)\n    validation = pd.read_csv(directory / \"validation\" / \"validation.csv\", header=None)\n    test = pd.read_csv(directory / \"test\" / \"test.csv\", header=None)\n\n    # After transforming the data, the number of features should be 7:\n    # * 3 - island (one-hot encoded)\n    # * 1 - culmen_length_mm = 1\n    # * 1 - culmen_depth_mm\n    # * 1 - flipper_length_mm\n    # * 1 - body_mass_g\n    number_of_features = 7\n\n    # The transformed splits should have an additional column for the target\n    # variable.\n    assert train.shape[1] == number_of_features + 1\n    assert validation.shape[1] == number_of_features + 1\n    assert test.shape[1] == number_of_features + 1\n\n\ndef test_train_baseline_is_not_transformed(directory):\n    baseline = pd.read_csv(\n        directory / \"train-baseline\" / \"train-baseline.csv\",\n        header=None,\n    )\n\n    island = baseline.iloc[:, 0].unique()\n\n    assert \"Biscoe\" in island\n    assert \"Torgersen\" in island\n    assert \"Dream\" in island\n\n\ndef test_test_baseline_is_not_transformed(directory):\n    baseline = pd.read_csv(\n        directory / \"test-baseline\" / \"test-baseline.csv\", header=None\n    )\n\n    island = baseline.iloc[:, 1].unique()\n\n    assert \"Biscoe\" in island\n    assert \"Torgersen\" in island\n    assert \"Dream\" in island\n\n\ndef test_train_baseline_includes_header(directory):\n    baseline = pd.read_csv(directory / \"train-baseline\" / \"train-baseline.csv\")\n    assert baseline.columns[0] == \"island\"\n\n\ndef test_test_baseline_does_not_include_header(directory):\n    baseline = pd.read_csv(directory / \"test-baseline\" / \"test-baseline.csv\")\n    assert baseline.columns[0] != \"island\"\n\n\n\n\nStep 2 - Caching Configuration\nSeveral SageMaker Pipeline steps support caching. When a step runs, and dependending on the configured caching policy, SageMaker will try to reuse the result of a previous successful run of the same step. You can find more information about this topic in Caching Pipeline Steps.\nLet’s define a caching policy that we’ll reuse on every step:\n\nfrom sagemaker.workflow.steps import CacheConfig\n\ncache_config = CacheConfig(enable_caching=True, expire_after=\"15d\")\n\n\n\nStep 3 - Pipeline Configuration\nWe can parameterize a SageMaker Pipeline to make it more flexible. In this case, we’ll use a parameter to pass the location of the dataset we want to process. We can execute the pipeline with different datasets by changing the value of this parameter. Check Pipeline Parameters for more information.\n\nfrom sagemaker.workflow.parameters import ParameterString\nfrom sagemaker.workflow.pipeline_definition_config import PipelineDefinitionConfig\n\npipeline_definition_config = PipelineDefinitionConfig(use_custom_job_prefix=True)\n\ndataset_location = ParameterString(\n    name=\"dataset_location\",\n    default_value=f\"{S3_LOCATION}/data\",\n)\n\n\n\nStep 4 - Setting up the Processing Step\nLet’s now define the ProcessingStep that we’ll use in the pipeline to run the script that will split and transform the data.\nA processor gives the Processing Step information about the hardware and software that SageMaker should use to launch a Processing Job. To run the script we created, we need access to Scikit-Learn, so we can use the SKLearnProcessor processor that comes out-of-the-box with the SageMaker’s Python SDK.\nSageMaker manages the infrastructure of a Processing Job. It provisions resources for the duration of the job, and cleans up when it completes. The Processing Container image that SageMaker uses to run a Processing Job can either be a SageMaker built-in image or a custom image:\n \nThe Data Processing with Framework Processors page discusses other built-in processors you can use. The Docker Registry Paths and Example Code page contains information about the available framework versions for each region.\n\nfrom sagemaker.sklearn.processing import SKLearnProcessor\n\nprocessor = SKLearnProcessor(\n    base_job_name=\"preprocess-data\",\n    framework_version=\"1.2-1\",\n    # By default, a new account doesn't have access to `ml.m5.xlarge` instances.\n    # If you haven't requested a quota increase yet, you can use an\n    # `ml.t3.medium` instance type instead. This will work out of the box, but\n    # the Processing Job will take significantly longer than it should have.\n    # To get access to `ml.m5.xlarge` instances, you can request a quota\n    # increase under the Service Quotas section in your AWS account.\n    instance_type=config[\"instance_type\"],\n    instance_count=1,\n    role=role,\n    sagemaker_session=config[\"session\"],\n)\n\nLet’s now define the Processing Step that we’ll use in the pipeline.\nThis step will specify the list of inputs that we’ll access from the preprocessing script. In this case, the input is the dataset we stored in S3. We also have a few outputs that we want SageMaker to capture when the Processing Job finishes.\n\nfrom sagemaker.processing import ProcessingInput, ProcessingOutput\nfrom sagemaker.workflow.steps import ProcessingStep\n\npreprocessing_step = ProcessingStep(\n    name=\"preprocess-data\",\n    step_args=processor.run(\n        code=f\"{(CODE_FOLDER / 'processing' / 'script.py').as_posix()}\",\n        inputs=[\n            ProcessingInput(\n                source=dataset_location,\n                destination=\"/opt/ml/processing/input\",\n            ),\n        ],\n        outputs=[\n            ProcessingOutput(\n                output_name=\"train\",\n                source=\"/opt/ml/processing/train\",\n                destination=f\"{S3_LOCATION}/preprocessing/train\",\n            ),\n            ProcessingOutput(\n                output_name=\"validation\",\n                source=\"/opt/ml/processing/validation\",\n                destination=f\"{S3_LOCATION}/preprocessing/validation\",\n            ),\n            ProcessingOutput(\n                output_name=\"test\",\n                source=\"/opt/ml/processing/test\",\n                destination=f\"{S3_LOCATION}/preprocessing/test\",\n            ),\n            ProcessingOutput(\n                output_name=\"model\",\n                source=\"/opt/ml/processing/model\",\n                destination=f\"{S3_LOCATION}/preprocessing/model\",\n            ),\n            ProcessingOutput(\n                output_name=\"train-baseline\",\n                source=\"/opt/ml/processing/train-baseline\",\n                destination=f\"{S3_LOCATION}/preprocessing/train-baseline\",\n            ),\n            ProcessingOutput(\n                output_name=\"test-baseline\",\n                source=\"/opt/ml/processing/test-baseline\",\n                destination=f\"{S3_LOCATION}/preprocessing/test-baseline\",\n            ),\n        ],\n    ),\n    cache_config=cache_config,\n)\n\n\n\nStep 5 - Creating the Pipeline\nWe can now create the SageMaker Pipeline and submit its definition to the SageMaker Pipelines service to create the pipeline if it doesn’t exist or update it if it does.\n\nfrom sagemaker.workflow.pipeline import Pipeline\n\nsession3_pipeline = Pipeline(\n    name=\"session3-pipeline\",\n    parameters=[dataset_location],\n    steps=[\n        preprocessing_step,\n    ],\n    pipeline_definition_config=pipeline_definition_config,\n    sagemaker_session=config[\"session\"],\n)\n\nsession3_pipeline.upsert(role_arn=role)",
    "crumbs": [
      "Building Machine Learning Systems That Don't Suck"
    ]
  },
  {
    "objectID": "cohort.html#session-4---training-the-model",
    "href": "cohort.html#session-4---training-the-model",
    "title": "Building Machine Learning Systems That Don’t Suck",
    "section": "Session 4 - Training the Model",
    "text": "Session 4 - Training the Model\nThis session extends the SageMaker Pipeline with a step to train a model. Check Train a Model with TensorFlow for more information about training a model with TensorFlow.\n \nWe’ll also introduce experiment tracking using Amazon SageMaker Experiments and Comet.\n\nStep 1 - Creating the Training Script\nLet’s create the training script. This script is responsible for training a neural network using the train data, validating the model, and saving it so we can later use it.\nWe will store the script in a folder called training and add it to the system path so we can later import it as a module.\n\n(CODE_FOLDER / \"training\").mkdir(parents=True, exist_ok=True)\nsys.path.extend([f\"./{CODE_FOLDER}/training\"])\n\nWe can now create the script inside the folder:\n\n\n\nscript.py\n\nimport argparse\nimport json\nimport os\nimport tarfile\nfrom pathlib import Path\n\nimport keras\nimport numpy as np\nimport pandas as pd\nfrom comet_ml import Experiment\nfrom keras import Input\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.optimizers import SGD\nfrom packaging import version\nfrom sklearn.metrics import accuracy_score\n\n\ndef train(\n    model_directory,\n    train_path,\n    validation_path,\n    pipeline_path,\n    experiment,\n    epochs=50,\n    batch_size=32,\n):\n    print(f\"Keras version: {keras.__version__}\")\n\n    X_train = pd.read_csv(Path(train_path) / \"train.csv\")\n    y_train = X_train[X_train.columns[-1]]\n    X_train = X_train.drop(X_train.columns[-1], axis=1)\n\n    X_validation = pd.read_csv(Path(validation_path) / \"validation.csv\")\n    y_validation = X_validation[X_validation.columns[-1]]\n    X_validation = X_validation.drop(X_validation.columns[-1], axis=1)\n\n    model = Sequential(\n        [\n            Input(shape=(X_train.shape[1],)),\n            Dense(10, activation=\"relu\"),\n            Dense(8, activation=\"relu\"),\n            Dense(3, activation=\"softmax\"),\n        ]\n    )\n\n    model.compile(\n        optimizer=SGD(learning_rate=0.01),\n        loss=\"sparse_categorical_crossentropy\",\n        metrics=[\"accuracy\"],\n    )\n\n    model.fit(\n        X_train,\n        y_train,\n        validation_data=(X_validation, y_validation),\n        epochs=epochs,\n        batch_size=batch_size,\n        verbose=2,\n    )\n\n    predictions = np.argmax(model.predict(X_validation), axis=-1)\n    val_accuracy = accuracy_score(y_validation, predictions)\n    print(f\"Validation accuracy: {val_accuracy}\")\n\n    # Starting on version 3, Keras changed the model saving format.\n    # Since we are running the training script using two different versions\n    # of Keras, we need to check to see which version we are using and save\n    # the model accordingly.\n    model_filepath = (\n        Path(model_directory) / \"001\"\n        if version.parse(keras.__version__) &lt; version.parse(\"3\")\n        else Path(model_directory) / \"penguins.keras\"\n    )\n\n    model.save(model_filepath)\n\n    # Let's save the transformation pipelines inside the\n    # model directory so they get bundled together.\n    with tarfile.open(Path(pipeline_path) / \"model.tar.gz\", \"r:gz\") as tar:\n        tar.extractall(model_directory)\n\n    if experiment:\n        experiment.log_parameters(\n            {\n                \"epochs\": epochs,\n                \"batch_size\": batch_size,\n            }\n        )\n        experiment.log_dataset_hash(X_train)\n        experiment.log_confusion_matrix(\n            y_validation.astype(int), predictions.astype(int)\n        )\n        experiment.log_model(\"penguins\", model_filepath.as_posix())\n\n\nif __name__ == \"__main__\":\n    # Any hyperparameters provided by the training job are passed to\n    # the entry point as script arguments.\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--epochs\", type=int, default=50)\n    parser.add_argument(\"--batch_size\", type=int, default=32)\n    args, _ = parser.parse_known_args()\n\n    # Let's create a Comet experiment to log the metrics and parameters\n    # of this training job.\n    comet_api_key = os.environ.get(\"COMET_API_KEY\", None)\n    comet_project_name = os.environ.get(\"COMET_PROJECT_NAME\", None)\n\n    experiment = (\n        Experiment(\n            project_name=comet_project_name,\n            api_key=comet_api_key,\n            auto_metric_logging=True,\n            auto_param_logging=True,\n            log_code=True,\n        )\n        if comet_api_key and comet_project_name\n        else None\n    )\n\n    training_env = json.loads(os.environ.get(\"SM_TRAINING_ENV\", {}))\n    job_name = training_env.get(\"job_name\", None) if training_env else None\n\n    # We want to use the SageMaker's training job name as the name\n    # of the experiment so we can easily recognize it.\n    if job_name and experiment:\n        experiment.set_name(job_name)\n\n    train(\n        # This is the location where we need to save our model.\n        # SageMaker will create a model.tar.gz file with anything\n        # inside this directory when the training script finishes.\n        model_directory=os.environ[\"SM_MODEL_DIR\"],\n        # SageMaker creates one channel for each one of the inputs\n        # to the Training Step.\n        train_path=os.environ[\"SM_CHANNEL_TRAIN\"],\n        validation_path=os.environ[\"SM_CHANNEL_VALIDATION\"],\n        pipeline_path=os.environ[\"SM_CHANNEL_PIPELINE\"],\n        experiment=experiment,\n        epochs=args.epochs,\n        batch_size=args.batch_size,\n    )\n\n\nLet’s test the script to ensure everything is working as expected:\n\n\nCode\nimport os\nimport shutil\nimport pytest\nimport tempfile\n\nfrom processing.script import preprocess\nfrom training.script import train\n\n@pytest.fixture(scope=\"function\", autouse=False)\ndef directory():\n    directory = tempfile.mkdtemp()\n    input_directory = Path(directory) / \"input\"\n    input_directory.mkdir(parents=True, exist_ok=True)\n    shutil.copy2(DATA_FILEPATH, input_directory / \"data.csv\")\n    \n    directory = Path(directory)\n    \n    preprocess(base_directory=directory)\n    train(\n        model_directory=directory / \"model\",\n        train_path=directory / \"train\", \n        validation_path=directory / \"validation\",\n        pipeline_path=directory / \"model\",\n        experiment=None,\n        epochs=1\n    )\n    \n    yield directory\n    \n    shutil.rmtree(directory)\n\n\ndef test_train_bundles_model_assets(directory):\n    bundle = os.listdir(directory / \"model\")\n    assert \"001\" in bundle\n    \n    assets = os.listdir(directory / \"model\" / \"001\")\n    assert \"saved_model.pb\" in assets\n\n\ndef test_train_bundles_transformation_pipelines(directory):\n    bundle = os.listdir(directory / \"model\")\n    assert \"target.joblib\" in bundle\n    assert \"features.joblib\" in bundle\n\n\n\n\nStep 2 - Setting up the Training Step\nWe can now create a Training Step that we can add to the pipeline. This Training Step will create a SageMaker Training Job in the background, run the training script, and upload the output to S3. Check the TrainingStep SageMaker’s SDK documentation for more information.\nSageMaker manages the infrastructure of a Training Job. It provisions resources for the duration of the job, and cleans up when it completes. The Training Container image that SageMaker uses to run a Training Job can either be a SageMaker built-in image or a custom image.\n \nThe Available Deep Learning Container Images page contains the list of available containers for each region.\nOur training script uses Comet to track metrics from the Training Job. We need to create a requirements.txt file to install the Comet library in the training container.\n\n\n\nrequirements.txt\n\ncomet_ml\n\n\nSageMaker uses the concept of an Estimator to handle end-to-end training and deployment tasks. For this example, we will use the built-in TensorFlow Estimator to run the training script we wrote before.\nNotice the list of hyperparameters defined below. SageMaker will pass these hyperparameters as arguments to the entry point of the training script.\nWe are going to use Comet and SageMaker Experiments to track metrics from the Training Job. SageMaker Experiments will use the list of metric definitions to decide which metrics to track and how to parse them from the Training Job logs. For more information, check Manage Machine Learning with Amazon SageMaker Experiments and the SageMaker Experiments’ SDK documentation.\nHere are the environment variables we need to set on the traininng container:\n\nCOMET_API_KEY: This is your Comet API key.\nCOMET_PROJECT_NAME: The name of the project where you want to track the experiments.\n\n\nfrom sagemaker.tensorflow import TensorFlow\n\nestimator = TensorFlow(\n    base_job_name=\"training\",\n    entry_point=\"script.py\",\n    source_dir=f\"{(CODE_FOLDER / 'training').as_posix()}\",\n    # SageMaker will pass these hyperparameters as arguments\n    # to the entry point of the training script.\n    hyperparameters={\n        \"epochs\": 50,\n        \"batch_size\": 32,\n    },\n    # SageMaker will create these environment variables on the\n    # Training Job instance.\n    environment={\n        \"COMET_API_KEY\": COMET_API_KEY,\n        \"COMET_PROJECT_NAME\": COMET_PROJECT_NAME,\n    },\n    # SageMaker will track these metrics as part of the experiment\n    # associated to this pipeline. The metric definitions tells\n    # SageMaker how to parse the values from the Training Job logs.\n    metric_definitions=[\n        {\"Name\": \"loss\", \"Regex\": \"loss: ([0-9\\\\.]+)\"},\n        {\"Name\": \"accuracy\", \"Regex\": \"accuracy: ([0-9\\\\.]+)\"},\n        {\"Name\": \"val_loss\", \"Regex\": \"val_loss: ([0-9\\\\.]+)\"},\n        {\"Name\": \"val_accuracy\", \"Regex\": \"val_accuracy: ([0-9\\\\.]+)\"},\n    ],\n    image_uri=config[\"image\"],\n    framework_version=config[\"framework_version\"],\n    py_version=config[\"py_version\"],\n    instance_type=config[\"instance_type\"],\n    instance_count=1,\n    disable_profiler=True,\n    debugger_hook_config=False,\n    sagemaker_session=config[\"session\"],\n    role=role,\n)\n\nWe can now create a Training Step. This Training Step will create a SageMaker Training Job in the background, run the training script, and upload the output to S3. Check the TrainingStep SageMaker’s SDK documentation for more information.\nThis step will receive the train and validation split from the preprocessing step as inputs.\nHere, we are using three input channels, train, validation, and pipeline. SageMaker will automatically create an environment variable corresponding to each of these channels following the format SM_CHANNEL_[channel_name]:\n\nSM_CHANNEL_TRAIN: This environment variable will contain the path to the training data coming from the preprocessing step.\nSM_CHANNEL_VALIDATION: This environment variable will contain the path to the validation data comimng from the preprocessing step.\nSM_CHANNEL_PIPELINE: This environment variable will contain the path to the transformation pipeline that we saved in the preprocessing step.\n\nNotice that we are creating a function that we can later reuse to create a training step using a different estimator.\n\nfrom sagemaker.inputs import TrainingInput\nfrom sagemaker.workflow.steps import TrainingStep\n\n\ndef create_training_step(estimator):\n    \"\"\"Create a SageMaker TrainingStep using the provided estimator.\"\"\"\n    return TrainingStep(\n        name=\"train-model\",\n        step_args=estimator.fit(\n            inputs={\n                \"train\": TrainingInput(\n                    s3_data=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\n                        \"train\"\n                    ].S3Output.S3Uri,\n                    content_type=\"text/csv\",\n                ),\n                \"validation\": TrainingInput(\n                    s3_data=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\n                        \"validation\"\n                    ].S3Output.S3Uri,\n                    content_type=\"text/csv\",\n                ),\n                \"pipeline\": TrainingInput(\n                    s3_data=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\n                        \"model\"\n                    ].S3Output.S3Uri,\n                    content_type=\"application/tar+gzip\",\n                ),\n            },\n        ),\n        cache_config=cache_config,\n    )\n\n\ntrain_model_step = create_training_step(estimator)\n\n\n\nStep 3 - Creating the Pipeline\nLet’s define the SageMaker Pipeline and submit its definition to the SageMaker Pipelines service to create the pipeline if it doesn’t exist or update it if it does.\n\nsession4_pipeline = Pipeline(\n    name=\"session4-pipeline\",\n    parameters=[dataset_location],\n    steps=[\n        preprocessing_step,\n        train_model_step,\n    ],\n    pipeline_definition_config=pipeline_definition_config,\n    sagemaker_session=config[\"session\"],\n)\n\nsession4_pipeline.upsert(role_arn=role)",
    "crumbs": [
      "Building Machine Learning Systems That Don't Suck"
    ]
  },
  {
    "objectID": "cohort.html#session-5---custom-training-container",
    "href": "cohort.html#session-5---custom-training-container",
    "title": "Building Machine Learning Systems That Don’t Suck",
    "section": "Session 5 - Custom Training Container",
    "text": "Session 5 - Custom Training Container\nThis session creates a custom Docker image to train the model and have full control of the environment where the training script runs.\nFor this example, we’ll run the training script using Keras 3 with a JAX backend. Check Adapting your own Docker container to work with SageMaker for more information about using your own Docker containers.\n\nStep 1 - Preparing the Docker Image\nThe first step is to copy the training script to a folder where we’ll prepare the Docker image. We are going to reuse the training script we created before, since it’s compatible with the latest version of Keras.\n\nimport shutil\n\n(CODE_FOLDER / \"containers\" / \"training\").mkdir(parents=True, exist_ok=True)\nshutil.copy2(\n    CODE_FOLDER / \"training\" / \"script.py\",\n    CODE_FOLDER / \"containers\" / \"training\" / \"train.py\",\n)\n\nPosixPath('code/containers/training/train.py')\n\n\nSince we are creating a new Docker image, we need to install the libraries we need in the training container. We’ll use a requirements.txt file to install these libraries. Notice that we are installing jax to run it as our backend.\nThe sagemaker-training library contains the common functionality necessary to create a container compatible with SageMaker and its Python SDK.\n\n\n\nrequirements.txt\n\nsagemaker-training\npackaging\nkeras\npandas\nscikit-learn\ncomet_ml\njax[cpu]\n\n\nWe can now create the Dockerfile containing the instructions to build the training image. Notice how this image will automatically run the train.py script when it starts.\nTo use JAX as the backend for our model, we need to set the KERAS_BACKEND environment variable to jax.\n\n\n\nDockerfile\n\nFROM python:3.10-slim\n\nRUN apt-get -y update && apt-get install -y --no-install-recommends \\\n    python3 \\\n    build-essential \\\n    libssl-dev\n\n# Let's install the required Python packages from \n# the requirements.txt file.\nCOPY requirements.txt .\nRUN pip install --user --upgrade pip\nRUN pip3 install -r requirements.txt\n\n# We are going to be running the training script\n# as the entrypoint of this container.\nCOPY train.py /opt/ml/code/train.py\nENV SAGEMAKER_PROGRAM train.py\n\n# We want to use JAX as the backend for Keras.\nENV KERAS_BACKEND=jax\n\n\n\n\nStep 2 - Building the Docker Image\nWe can now build the Docker image using the docker build command. We are going to define the name of this image using the IMAGE_NAME variable.\n\nIMAGE_NAME = \"keras-custom-training-container\"\n\nif not LOCAL_MODE:\n    # If we aren't running the code in Local Mode, we need\n    # to specify we want to build the Docker image for the\n    # linux/amd64 architecture before uploading it to ECR.\n    print(\"Building Docker image for linux/amd64 architecture...\")\n\n    !docker build --platform=\"linux/amd64\" -t $IMAGE_NAME \\\n        $CODE_FOLDER/containers/training/\nelse:\n    # If we are running in Local Mode, we can use the\n    # default Docker build command.\n    print(\"Building Docker image for arm64 architecture...\")\n\n    !docker build -t $IMAGE_NAME \\\n        $CODE_FOLDER/containers/training/\n\n\n\nStep 3 - Pushing Docker Image to ECR\nWe can now push the Docker image to Amazon Elastic Container Registry (ECR). This is a fully-managed Docker container registry where we can manage Docker container images. This step is necessary to make the image available to SageMaker when running the pipeline.\n\nalgorithm_name=$2\naccount=$(aws sts get-caller-identity --query Account --output text)\n\n# Get the region defined in the current configuration\n# (default to us-east-1 if none defined)\nregion=$(aws configure get region)\nregion=${region:-us-east-1}\n\nrepository=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n\n# We only want to push the Docker image to ECR if\n# we are not running in Local Mode.\nif [ $1 = \"False\" ]\nthen\n    # Create the repository if it doesn't exist in ECR\n    aws ecr describe-repositories \\\n        --repository-names \"${algorithm_name}\" &gt; /dev/null 2&gt;&1\n    if [ $? -ne 0 ]\n    then\n        aws ecr create-repository \\\n            --repository-name \"${algorithm_name}\" &gt; /dev/null\n    fi\n\n    # Get the login command from ECR to run the\n    # Docker push command.\n    aws ecr get-login-password \\\n        --region ${region}|docker \\\n        login --username AWS --password-stdin ${repository}\n\n    # Push the Docker image to the ECR repository\n    docker tag ${algorithm_name} ${repository}\n    docker push ${repository}\nfi\n\n\n\nStep 4 - Setting up the Training Step\nLet’s now compute the name of the training image we’ll use to run the Training Job.\nIf we are running in LOCAL_MODE, we’ll use the name of the image we built before (IMAGE_NAME). Otherwise, we’ll use the name of the image we pushed to ECR.\n\naccount_id = boto3.client(\"sts\").get_caller_identity().get(\"Account\")\ntag = \":latest\"\n\nuri_suffix = \"amazonaws.com\"\nif region in [\"cn-north-1\", \"cn-northwest-1\"]:\n    uri_suffix = \"amazonaws.com.cn\"\n\ntraining_container_image = (\n    IMAGE_NAME\n    if LOCAL_MODE\n    else (f\"{account_id}.dkr.ecr.{region}.amazonaws.com/{IMAGE_NAME}:latest\")\n)\n\ntraining_container_image\n\n'keras-custom-training-container'\n\n\nWe can now create an Estimator and a Training Step using the function we created before.\n\nfrom sagemaker.estimator import Estimator\n\nkeras_estimator = Estimator(\n    image_uri=training_container_image,\n    instance_count=1,\n    instance_type=config[\"instance_type\"],\n    sagemaker_session=config[\"session\"],\n    role=role,\n)\n\nkeras_train_model_step = create_training_step(keras_estimator)\n\n\n\nStep 5 - Creating the Pipeline\nLet’s define the SageMaker Pipeline and submit its definition to the SageMaker Pipelines service to create the pipeline if it doesn’t exist or update it if it does.\n\nsession5_pipeline = Pipeline(\n    name=\"session5-pipeline\",\n    parameters=[dataset_location],\n    steps=[\n        preprocessing_step,\n        # This time we want to use the new training step\n        # we created using the custom Docker image.\n        keras_train_model_step,\n    ],\n    pipeline_definition_config=pipeline_definition_config,\n    sagemaker_session=config[\"session\"],\n)\n\nsession5_pipeline.upsert(role_arn=role)",
    "crumbs": [
      "Building Machine Learning Systems That Don't Suck"
    ]
  },
  {
    "objectID": "cohort.html#session-6---tuning-the-model",
    "href": "cohort.html#session-6---tuning-the-model",
    "title": "Building Machine Learning Systems That Don’t Suck",
    "section": "Session 6 - Tuning the Model",
    "text": "Session 6 - Tuning the Model\nThis session extends the SageMaker Pipeline with a step to tune the model using a Hyperparameter Tuning Job.\n \n\nStep 1 - Enabling Tuning\nSince we could use the Training of the Tuning Step to create a model, we’ll define a constant to indicate which approach we want to run. Notice that the Tuning Step is not supported in Local Mode.\n\nUSE_TUNING_STEP = False\n\n\n\nStep 2 - Setting up a Tuning Step\nLet’s now create a Tuning Step. This Tuning Step will create a SageMaker Hyperparameter Tuning Job in the background and use the training script to train different model variants and choose the best one. Check the TuningStep SageMaker’s SDK documentation for more information.\nThe Tuning Step requires a HyperparameterTuner reference to configure the Hyperparameter Tuning Job.\nHere is the configuration that we’ll use to find the best model:\n\nobjective_metric_name: This is the name of the metric the tuner will use to determine the best model.\nobjective_type: This is the objective of the tuner. It specifies whether it should minimize the metric or maximize it. In this example, since we are using the validation accuracy of the model, we want the objective to be “Maximize.” If we were using the loss of the model, we would set the objective to “Minimize.”\nmetric_definitions: Defines how the tuner will determine the metric’s value by looking at the output logs of the training process.\n\nThe tuner expects the list of the hyperparameters you want to explore. You can use subclasses of the Parameter class to specify different types of hyperparameters. This example explores different values for the epochs hyperparameter.\nFinally, you can control the number of jobs and how many of them will run in parallel using the following two arguments:\n\nmax_jobs: Defines the maximum total number of training jobs to start for the hyperparameter tuning job.\nmax_parallel_jobs: Defines the maximum number of parallel training jobs to start.\n\n\nfrom sagemaker.parameter import IntegerParameter\nfrom sagemaker.tuner import HyperparameterTuner\n\ntuner = HyperparameterTuner(\n    estimator,\n    objective_metric_name=\"val_accuracy\",\n    objective_type=\"Maximize\",\n    hyperparameter_ranges={\n        \"epochs\": IntegerParameter(10, 50),\n    },\n    metric_definitions=[{\"Name\": \"val_accuracy\", \"Regex\": \"val_accuracy: ([0-9\\\\.]+)\"}],\n    max_jobs=3,\n    max_parallel_jobs=3,\n)\n\nWe can now create the Tuning Step using the tuner we configured before. SageMaker will create a Hyperparameter Tuning Job in the background and use the training script to train different model variants and choose the best one.\n \n\nfrom sagemaker.workflow.steps import TuningStep\n\ntune_model_step = TuningStep(\n    name=\"tune-model\",\n    step_args=tuner.fit(\n        inputs={\n            \"train\": TrainingInput(\n                s3_data=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\n                    \"train\"\n                ].S3Output.S3Uri,\n                content_type=\"text/csv\",\n            ),\n            \"validation\": TrainingInput(\n                s3_data=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\n                    \"validation\"\n                ].S3Output.S3Uri,\n                content_type=\"text/csv\",\n            ),\n            \"pipeline\": TrainingInput(\n                s3_data=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\n                    \"model\"\n                ].S3Output.S3Uri,\n                content_type=\"application/tar+gzip\",\n            ),\n        },\n    ),\n    cache_config=cache_config,\n)\n\n\n\nStep 3 - Creating the Pipeline\nLet’s define the SageMaker Pipeline and submit its definition to the SageMaker Pipelines service to create the pipeline if it doesn’t exist or update it if it does.\n\nsession6_pipeline = Pipeline(\n    name=\"session6-pipeline\",\n    parameters=[dataset_location],\n    steps=[\n        preprocessing_step,\n        tune_model_step,\n    ],\n    pipeline_definition_config=pipeline_definition_config,\n    sagemaker_session=config[\"session\"],\n)\n\nsession6_pipeline.upsert(role_arn=role)",
    "crumbs": [
      "Building Machine Learning Systems That Don't Suck"
    ]
  },
  {
    "objectID": "cohort.html#session-7---evaluating-the-model",
    "href": "cohort.html#session-7---evaluating-the-model",
    "title": "Building Machine Learning Systems That Don’t Suck",
    "section": "Session 7 - Evaluating the Model",
    "text": "Session 7 - Evaluating the Model\nThis session extends the SageMaker Pipeline with a step to evaluate the model using the holdout set we created during the preprocessing step.\n \n\nStep 1 - Creating the Evaluation Script\nWe’ll use a Processing Step to execute the evaluation script.\nThis script is responsible for loading the model we created and evaluating it on the test set. Before finishing, this script will generate an evaluation report of the model.\nWe will store the script in a folder called evaluation and add it to the system path so we can later import it as a module.\n\n(CODE_FOLDER / \"evaluation\").mkdir(parents=True, exist_ok=True)\nsys.path.extend([f\"./{CODE_FOLDER}/evaluation\"])\n\nWe can now create the script inside the folder:\n\n\n\nscript.py\n\nimport json\nimport tarfile\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\nfrom tensorflow import keras\n\n\ndef evaluate(model_path, test_path, output_path):\n    X_test = pd.read_csv(Path(test_path) / \"test.csv\")\n    y_test = X_test[X_test.columns[-1]]\n    X_test = X_test.drop(X_test.columns[-1], axis=1)\n\n    # Let's now extract the model package so we can load\n    # it in memory.\n    with tarfile.open(Path(model_path) / \"model.tar.gz\") as tar:\n        tar.extractall(path=Path(model_path))\n\n    model = keras.models.load_model(Path(model_path) / \"001\")\n\n    predictions = np.argmax(model.predict(X_test), axis=-1)\n    accuracy = accuracy_score(y_test, predictions)\n    print(f\"Test accuracy: {accuracy}\")\n\n    # Let's create an evaluation report using the model accuracy.\n    evaluation_report = {\n        \"metrics\": {\n            \"accuracy\": {\"value\": accuracy},\n        },\n    }\n\n    Path(output_path).mkdir(parents=True, exist_ok=True)\n    with open(Path(output_path) / \"evaluation.json\", \"w\") as f:\n        f.write(json.dumps(evaluation_report))\n\n\nif __name__ == \"__main__\":\n    evaluate(\n        model_path=\"/opt/ml/processing/model/\",\n        test_path=\"/opt/ml/processing/test/\",\n        output_path=\"/opt/ml/processing/evaluation/\",\n    )\n\n\nLet’s test the script to ensure everything is working as expected:\n\n\nCode\nimport os\nimport shutil\nimport tarfile\nimport pytest\nimport tempfile\n\nfrom processing.script import preprocess\nfrom training.script import train\nfrom evaluation.script import evaluate\n\n\n@pytest.fixture(scope=\"function\", autouse=False)\ndef directory():\n    directory = tempfile.mkdtemp()\n    input_directory = Path(directory) / \"input\"\n    input_directory.mkdir(parents=True, exist_ok=True)\n    shutil.copy2(DATA_FILEPATH, input_directory / \"data.csv\")\n\n    directory = Path(directory)\n\n    preprocess(base_directory=directory)\n\n    train(\n        model_directory=directory / \"model\",\n        train_path=directory / \"train\",\n        validation_path=directory / \"validation\",\n        pipeline_path=directory / \"model\",\n        experiment=None,\n        epochs=1,\n    )\n\n    # After training a model, we need to prepare a package just like\n    # SageMaker would. This package is what the evaluation script is\n    # expecting as an input.\n    with tarfile.open(directory / \"model.tar.gz\", \"w:gz\") as tar:\n        tar.add(directory / \"model\" / \"001\", arcname=\"001\")\n\n    evaluate(\n        model_path=directory,\n        test_path=directory / \"test\",\n        output_path=directory / \"evaluation\",\n    )\n\n    yield directory / \"evaluation\"\n\n    shutil.rmtree(directory)\n\n\ndef test_evaluate_generates_evaluation_report(directory):\n    output = os.listdir(directory)\n    assert \"evaluation.json\" in output\n\n\ndef test_evaluation_report_contains_accuracy(directory):\n    with open(directory / \"evaluation.json\", \"r\") as file:\n        report = json.load(file)\n\n    assert \"metrics\" in report\n    assert \"accuracy\" in report[\"metrics\"]\n\n\n\n\nStep 2 - Referencing the Model Assets\nOne of the inputs to the evaluation step is the model coming from the Training or the Tuning step. We can use the USE_TUNING_STEP flag to determine whether we created the model using a Training Step or a Tuning Step.\nIn case we are using the Tuning Step, we can use the TuningStep.get_top_model_s3_uri() function to get the model assets from the top performing training job of the Hyperparameter Tuning Job.\n\nmodel_assets = train_model_step.properties.ModelArtifacts.S3ModelArtifacts\n\nif USE_TUNING_STEP:\n    model_assets = tune_model_step.get_top_model_s3_uri(\n        top_k=0,\n        s3_bucket=config[\"session\"].default_bucket(),\n    )\n\n\n\nStep 3 - Mapping the Output to a Property File\nSageMaker supports mapping outputs from a Processing Step to property files. This is useful when we want to access a specific property from the pipeline.\nWe’ll map the evaluation report to a property file. Check How to Build and Manage Property Files for more information.\n\nfrom sagemaker.workflow.properties import PropertyFile\n\nevaluation_report = PropertyFile(\n    name=\"evaluation-report\",\n    output_name=\"evaluation\",\n    path=\"evaluation.json\",\n)\n\n\n\nStep 4 - Setting up the Evaluation Step\nTo run the evaluation script, we will use a Processing Step configured with a TensorFlowProcessor because the script needs access to TensorFlow.\n\nfrom sagemaker.tensorflow import TensorFlowProcessor\n\nevaluation_processor = TensorFlowProcessor(\n    base_job_name=\"evaluation-processor\",\n    image_uri=config[\"image\"],\n    framework_version=config[\"framework_version\"],\n    py_version=config[\"py_version\"],\n    instance_type=config[\"instance_type\"],\n    instance_count=1,\n    role=role,\n    sagemaker_session=config[\"session\"],\n)\n\nWe are now ready to define the Processing Step that will run the evaluation script:\n\nevaluate_model_step = ProcessingStep(\n    name=\"evaluate-model\",\n    step_args=evaluation_processor.run(\n        code=f\"{(CODE_FOLDER / 'evaluation' / 'script.py').as_posix()}\",\n        inputs=[\n            # The first input is the test split that we generated on\n            # the first step of the pipeline when we split and\n            # transformed the data.\n            ProcessingInput(\n                source=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\n                    \"test\"\n                ].S3Output.S3Uri,\n                destination=\"/opt/ml/processing/test\",\n            ),\n            # The second input is the model that we generated on\n            # the Training or Tunning Step.\n            ProcessingInput(\n                source=model_assets,\n                destination=\"/opt/ml/processing/model\",\n            ),\n        ],\n        outputs=[\n            # The output is the evaluation report that we generated\n            # in the evaluation script.\n            ProcessingOutput(\n                output_name=\"evaluation\",\n                source=\"/opt/ml/processing/evaluation\",\n            ),\n        ],\n    ),\n    property_files=[evaluation_report],\n    cache_config=cache_config,\n)\n\n\n\nStep 5 - Creating the Pipeline\nLet’s define the SageMaker Pipeline and submit its definition to the SageMaker Pipelines service to create the pipeline if it doesn’t exist or update it if it does.\n\nsession7_pipeline = Pipeline(\n    name=\"session7-pipeline\",\n    parameters=[dataset_location],\n    steps=[\n        preprocessing_step,\n        tune_model_step if USE_TUNING_STEP else train_model_step,\n        evaluate_model_step,\n    ],\n    pipeline_definition_config=pipeline_definition_config,\n    sagemaker_session=config[\"session\"],\n)\n\nsession7_pipeline.upsert(role_arn=role)",
    "crumbs": [
      "Building Machine Learning Systems That Don't Suck"
    ]
  },
  {
    "objectID": "cohort.html#session-8---registering-the-model",
    "href": "cohort.html#session-8---registering-the-model",
    "title": "Building Machine Learning Systems That Don’t Suck",
    "section": "Session 8 - Registering the Model",
    "text": "Session 8 - Registering the Model\nThis session extends the SageMaker Pipeline with a step to register the model in the SageMaker Model Registry.\n \n\nStep 1 - Configuring the Model Package Group\nFirst, let’s define the name of the group where we’ll register the model. The Model Registry uses groups to organize the versions of a model:\n\nBASIC_MODEL_PACKAGE_GROUP = \"basic-penguins\"\n\n\n\nStep 2 - Creating the Model\nLet’s now create the model that we’ll register in the Model Registry. The model we trained uses TensorFlow, so we can use the built-in TensorFlowModel class to create an instance of the model:\n\nfrom sagemaker.tensorflow.model import TensorFlowModel\n\ntensorflow_model = TensorFlowModel(\n    model_data=model_assets,\n    framework_version=config[\"framework_version\"],\n    sagemaker_session=config[\"session\"],\n    role=role,\n)\n\n\n\nStep 3 - Configuring Model Metrics\nWhen we register a model in the Model Registry, we can attach relevant metadata to it. We’ll use the evaluation report we generated during the evaluation step to populate the metrics of this model:\n\nfrom sagemaker.model_metrics import MetricsSource, ModelMetrics\nfrom sagemaker.workflow.functions import Join\n\nmodel_metrics = ModelMetrics(\n    model_statistics=MetricsSource(\n        s3_uri=Join(\n            on=\"/\",\n            values=[\n                evaluate_model_step.properties.ProcessingOutputConfig.Outputs[\n                    \"evaluation\"\n                ].S3Output.S3Uri,\n                \"evaluation.json\",\n            ],\n        ),\n        content_type=\"application/json\",\n    ),\n)\n\n\n\nStep 4 - Registering the Model\nWe can use a Model Step to register the model. Check the ModelStep SageMaker’s SDK documentation for more information.\n\nfrom sagemaker.workflow.model_step import ModelStep\n\n\ndef create_registration_step(\n    model,\n    model_package_group_name,\n    approval_status=\"Approved\",\n    content_types=[\"text/csv\"],\n    response_types=[\"application/json\"],\n    model_metrics=None,\n    drift_check_baselines=None,\n):\n    \"\"\"Create a Registration Step using the supplied parameters.\"\"\"\n    return ModelStep(\n        name=\"register\",\n        step_args=model.register(\n            model_package_group_name=model_package_group_name,\n            approval_status=approval_status,\n            model_metrics=model_metrics,\n            drift_check_baselines=drift_check_baselines,\n            content_types=content_types,\n            response_types=response_types,\n            inference_instances=[config[\"instance_type\"]],\n            transform_instances=[config[\"instance_type\"]],\n            framework_version=config[\"framework_version\"],\n            domain=\"MACHINE_LEARNING\",\n            task=\"CLASSIFICATION\",\n            framework=\"TENSORFLOW\",\n        ),\n    )\n\n\nregister_model_step = create_registration_step(\n    tensorflow_model,\n    BASIC_MODEL_PACKAGE_GROUP,\n    model_metrics=model_metrics,\n)\n\n\n\nStep 5 - Creating the Pipeline\nLet’s define the SageMaker Pipeline and submit its definition to the SageMaker Pipelines service to create the pipeline if it doesn’t exist or update it if it does.\n\nsession8_pipeline = Pipeline(\n    name=\"session8-pipeline\",\n    parameters=[dataset_location],\n    steps=[\n        preprocessing_step,\n        tune_model_step if USE_TUNING_STEP else train_model_step,\n        evaluate_model_step,\n        register_model_step,\n    ],\n    pipeline_definition_config=pipeline_definition_config,\n    sagemaker_session=config[\"session\"],\n)\n\nsession8_pipeline.upsert(role_arn=role)",
    "crumbs": [
      "Building Machine Learning Systems That Don't Suck"
    ]
  },
  {
    "objectID": "cohort.html#session-9---conditional-registration",
    "href": "cohort.html#session-9---conditional-registration",
    "title": "Building Machine Learning Systems That Don’t Suck",
    "section": "Session 9 - Conditional Registration",
    "text": "Session 9 - Conditional Registration\nThis session extends the SageMaker Pipeline with a condition to register the model only if its accuracy is above a predefined threshold.\nHere’s a high-level overview of the Condition Step:\n \n\nStep 1 - Configuring the Accuracy Threshold\nLet’s define a new Pipeline Parameter to specify the minimum accuracy that the model should reach for it to be registered.\n\nfrom sagemaker.workflow.parameters import ParameterFloat\n\naccuracy_threshold = ParameterFloat(name=\"accuracy_threshold\", default_value=0.70)\n\n\n\nStep 2 - Setting up a Fail Step\nIf the model’s accuracy is not greater than or equal to our threshold, we will send the pipeline to a Fail Step with the appropriate error message. Check the FailStep SageMaker’s SDK documentation for more information.\n\nfrom sagemaker.workflow.fail_step import FailStep\n\nfail_step = FailStep(\n    name=\"fail\",\n    error_message=Join(\n        on=\" \",\n        values=[\n            \"Execution failed because the model's accuracy was lower than\",\n            accuracy_threshold,\n        ],\n    ),\n)\n\n\n\nStep 3 - Defining the Condition\nWe can use a ConditionGreaterThanOrEqualTo condition to compare the model’s accuracy with the threshold. Look at the Conditions section in the documentation for more information about the types of supported conditions.\n\nfrom sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\nfrom sagemaker.workflow.functions import JsonGet\n\ncondition = ConditionGreaterThanOrEqualTo(\n    left=JsonGet(\n        step_name=evaluate_model_step.name,\n        property_file=evaluation_report,\n        json_path=\"metrics.accuracy.value\",\n    ),\n    right=accuracy_threshold,\n)\n\n\n\nStep 4 - Setting up the Condition Step\nLet’s now use a Condition Step together with the evaluation report we generated to determine whether the model’s accuracy is above the threshold:\n\nfrom sagemaker.workflow.condition_step import ConditionStep\n\ncondition_step = ConditionStep(\n    name=\"check-model-accuracy\",\n    conditions=[condition],\n    if_steps=[register_model_step],\n    else_steps=[fail_step],\n)\n\n\n\nStep 5 - Creating the Pipeline\nWe can now define the SageMaker Pipeline and submit its definition to the SageMaker Pipelines service to create the pipeline if it doesn’t exist or update it if it does.\n\nsession9_pipeline = Pipeline(\n    name=\"session9-pipeline\",\n    parameters=[dataset_location, accuracy_threshold],\n    steps=[\n        preprocessing_step,\n        tune_model_step if USE_TUNING_STEP else train_model_step,\n        evaluate_model_step,\n        condition_step,\n    ],\n    pipeline_definition_config=pipeline_definition_config,\n    sagemaker_session=config[\"session\"],\n)\n\nsession9_pipeline.upsert(role_arn=role)",
    "crumbs": [
      "Building Machine Learning Systems That Don't Suck"
    ]
  },
  {
    "objectID": "cohort.html#session-10---serving-the-model",
    "href": "cohort.html#session-10---serving-the-model",
    "title": "Building Machine Learning Systems That Don’t Suck",
    "section": "Session 10 - Serving the Model",
    "text": "Session 10 - Serving the Model\nThis session builds a simple Flask application to serve the model.\n \nKeep in mind that, while good for development and testing, this is not the best approach for production systems.\n\nStep 1 - Retrieving List of Approved Models\nWe want to serve the latest approved model from the Model Registry. We can use the boto3 API to get this model:\n\nresponse = sagemaker_client.list_model_packages(\n    ModelPackageGroupName=BASIC_MODEL_PACKAGE_GROUP,\n    ModelApprovalStatus=\"Approved\",\n    SortBy=\"CreationTime\",\n    MaxResults=1,\n)\n\npackage = (\n    response[\"ModelPackageSummaryList\"][0]\n    if response[\"ModelPackageSummaryList\"]\n    else None\n)\n\npackage\n\n{'ModelPackageGroupName': 'basic-penguins',\n 'ModelPackageVersion': 6,\n 'ModelPackageArn': 'arn:aws:sagemaker:us-east-1:325223348818:model-package/basic-penguins/6',\n 'CreationTime': datetime.datetime(2024, 3, 29, 11, 19, 48, 782000, tzinfo=tzlocal()),\n 'ModelPackageStatus': 'Completed',\n 'ModelApprovalStatus': 'Approved'}\n\n\n\n\nStep 2 - Downloading the Model\nLet’s now download the model assets from the location specified in the Model Registry to your local environment.\nWe will store this model in a folder called serving:\n\n(CODE_FOLDER / \"serving\").mkdir(parents=True, exist_ok=True)\n\nLet’s now download the model assets into the folder:\n\nfrom sagemaker.s3 import S3Downloader\n\nif package:\n    response = sagemaker_client.describe_model_package(\n        ModelPackageName=package[\"ModelPackageArn\"],\n    )\n\n    model_data = response[\"InferenceSpecification\"][\"Containers\"][0][\"ModelDataUrl\"]\n    S3Downloader.download(model_data, (CODE_FOLDER / \"serving\").as_posix())\n\n\n\nStep 3 - Creating the Serving Script\nLet’s now write a simple Flask script to serve the model.\nWhen this application receives the first request, it will unpack and load the model into memory. From there, it will use the model to make predictions on the incoming requests.\n\n\n\napp.py\n\nimport tarfile\nimport tempfile\nimport numpy as np\n\nfrom flask import Flask, request, jsonify\nfrom pathlib import Path\nfrom tensorflow import keras\n\n\nMODEL_PATH = Path(__file__).parent\n\n\nclass Model:\n    model = None\n\n    def load(self):\n        \"\"\"\n        Extracts the model package and loads the model in memory\n        if it hasn't been loaded yet.\n        \"\"\"\n        # We want to load the model only if it is not loaded yet.\n        if not Model.model:\n            # Before we load the model, we need to extract it in\n            # a temporal directory.\n\n            with tempfile.TemporaryDirectory() as directory:\n                with tarfile.open(MODEL_PATH / \"model.tar.gz\") as tar:\n                    tar.extractall(path=directory)\n\n                Model.model = keras.models.load_model(Path(directory) / \"001\")\n\n    def predict(self, data):\n        \"\"\"\n        Generates predictions for the supplied data.\n        \"\"\"\n        self.load()\n        return Model.model.predict(data)\n\n\napp = Flask(__name__)\nmodel = Model()\n\n\n@app.route(\"/predict/\", methods=[\"POST\"])\ndef predict():\n    data = request.data.decode(\"utf-8\")\n\n    data = np.array(data.split(\",\")).astype(np.float32)\n    data = np.expand_dims(data, axis=0)\n\n    predictions = model.predict(data=[data])\n\n    prediction = int(np.argmax(predictions[0], axis=-1))\n    confidence = float(predictions[0][prediction])\n\n    return jsonify({\"prediction\": prediction, \"confidence\": confidence})\n\n\n\n\nStep 4 - Running the Flask Application\nWe can now run the Flask application to serve the model from a terminal using the following command:\n$ flask --app program/code/serving/app.py --debug run --host=0.0.0.0 --port=4242\nAfter the server is running, you can send a POST request to the server to get a prediction. Here is an example using the curl command:\n$ curl --location --request POST 'http://localhost:4242/predict' \\\n    --header 'Content-Type: text/plain' \\\n    --data-raw '0.6569590202313976, -1.0813829646495108, 1.2097102831892812, 0.9226343641317372, 1.0, 0.0, 0.0'",
    "crumbs": [
      "Building Machine Learning Systems That Don't Suck"
    ]
  },
  {
    "objectID": "cohort.html#session-11---deploying-the-model",
    "href": "cohort.html#session-11---deploying-the-model",
    "title": "Building Machine Learning Systems That Don’t Suck",
    "section": "Session 11 - Deploying the Model",
    "text": "Session 11 - Deploying the Model\nThis session deploys the model from the Model Registry to an endpoint. We’ll do it manually, using boto3 and the SageMaker SDK. Check Deploy to a SageMaker Endpoint for more information about deploying a model to an endpoint.\n \n\nStep 1 - Configuring the Endpoint Name\nLet’s start by defining the name of the endpoint where we’ll deploy the model:\n\nfrom sagemaker.predictor import Predictor\n\nENDPOINT = \"penguins-endpoint\"\n\n\n\nStep 2 - Creating a Model Package\nTo deploy a model using the SageMaker’s Python SDK, we need to create a Model Package using the ARN of the model from the Model Registry. Remember we got the ARN of the latest approved model in the previous section.\n\nfrom sagemaker import ModelPackage\n\nif package:\n    model_package = ModelPackage(\n        model_package_arn=package[\"ModelPackageArn\"],\n        sagemaker_session=sagemaker_session,\n        role=role,\n    )\n\n    print(package[\"ModelPackageArn\"])\n\narn:aws:sagemaker:us-east-1:325223348818:model-package/basic-penguins/6\n\n\n\n\nStep 3 - Deploying the Model\nLet’s now deploy the model to an endpoint.\n\nmodel_package.deploy(\n    endpoint_name=ENDPOINT,\n    initial_instance_count=1,\n    instance_type=config[\"instance_type\"],\n)\n\n\n\nStep 4 - Testing the Endpoint\nAfter deploying the model, we can test the endpoint to make sure it works.\nEach line of the payload we’ll send to the endpoint contains the information of a penguin. Notice the model expects data that’s already transformed. We can’t provide the original data from our dataset because the model we registered will not work with it.\nThe endpoint will return the predictions for each of these lines.\n\npayload = \"\"\"\n0.6569590202313976,-1.0813829646495108,1.2097102831892812,0.9226343641317372,1.0,0.0,0.0\n-0.7751048801481084,0.8822689351285553,-1.2168066120762704,0.9226343641317372,0.0,1.0,0.0\n-0.837387834894918,0.3386660813829646,-0.26237731892812,-1.92351941317372,0.0,0.0,1.0\n\"\"\"\n\nLet’s send the payload to the endpoint and print its response:\n\npredictor = Predictor(endpoint_name=ENDPOINT)\n\ntry:\n    response = predictor.predict(payload, initial_args={\"ContentType\": \"text/csv\"})\n    response = json.loads(response.decode(\"utf-8\"))\n\n    print(json.dumps(response, indent=2))\n    print(f\"\\nSpecies: {np.argmax(response['predictions'], axis=1)}\")\nexcept Exception as e:\n    print(e)\n\nAn error occurred (ValidationError) when calling the InvokeEndpoint operation: Endpoint penguins-endpoint of account 325223348818 not found.",
    "crumbs": [
      "Building Machine Learning Systems That Don't Suck"
    ]
  },
  {
    "objectID": "cohort.html#session-12---deploying-from-the-pipeline",
    "href": "cohort.html#session-12---deploying-from-the-pipeline",
    "title": "Building Machine Learning Systems That Don’t Suck",
    "section": "Session 12 - Deploying From the Pipeline",
    "text": "Session 12 - Deploying From the Pipeline\nThis session extends the SageMaker Pipeline with a step to automatically deploy the model to an endpoint.\nWe’ll use a Lambda Step to create an endpoint and deploy the model.\nHere’s a high-level overview of the Deploy Step:\n \n\nStep 1 - Configuring Data Capture Settings\nWe want to enable Data Capture as part of the endpoint configuration. With Data Capture we can record the inputs and outputs of the endpoint to use them later for monitoring the model. We need to configuration settings to enable Data Capture:\n\nDATA_CAPTURE_PERCENTAGE: Represents the percentage of traffic that we want to capture.\nDATA_CAPTURE_DESTINATION: Specifies the S3 location where we want to store the captured data.\n\n\nDATA_CAPTURE_PERCENTAGE = 100\nDATA_CAPTURE_DESTINATION = f\"{S3_LOCATION}/monitoring/data-capture\"\n\n\n\nStep 2 - Setting up the Lambda Function\nLet’s start by writing a Lambda function that takes the model information and deploys it to an endpoint.\nThere are three components that make up a SageMaker endpoint:\n \nWe’ll store the code of the function in a folder called lambda:\n\n(CODE_FOLDER / \"lambda\").mkdir(parents=True, exist_ok=True)\n\nLet’s now write the code of the function:\n\n\n\nlambda.py\n\nimport os\nimport json\nimport boto3\nimport time\n\nsagemaker = boto3.client(\"sagemaker\")\n\n\ndef lambda_handler(event, context):\n    # If we are calling this function from EventBridge,\n    # we need to extract the model package ARN and the\n    # approval status from the event details. If we are\n    # calling this function from the pipeline, we can\n    # assume the model is approved and we can get the\n    # model package ARN as a direct parameter.\n    if \"detail\" in event:\n        model_package_arn = event[\"detail\"][\"ModelPackageArn\"]\n        approval_status = event[\"detail\"][\"ModelApprovalStatus\"]\n    else:\n        model_package_arn = event[\"model_package_arn\"]\n        approval_status = \"Approved\"\n\n    print(f\"Model: {model_package_arn}\")\n    print(f\"Approval status: {approval_status}\")\n\n    if approval_status != \"Approved\":\n        response = {\n            \"message\": \"Skipping deployment.\",\n            \"approval_status\": approval_status,\n        }\n\n        print(response)\n        return {\"statusCode\": 200, \"body\": json.dumps(response)}\n\n    endpoint_name = os.environ[\"ENDPOINT\"]\n    data_capture_percentage = int(os.environ[\"DATA_CAPTURE_PERCENTAGE\"])\n    data_capture_destination = os.environ[\"DATA_CAPTURE_DESTINATION\"]\n    role = os.environ[\"ROLE\"]\n\n    timestamp = time.strftime(\"%m%d%H%M%S\", time.localtime())\n    model_name = f\"{endpoint_name}-model-{timestamp}\"\n    endpoint_config_name = f\"{endpoint_name}-config-{timestamp}\"\n\n    sagemaker.create_model(\n        ModelName=model_name,\n        ExecutionRoleArn=role,\n        Containers=[{\"ModelPackageName\": model_package_arn}],\n    )\n\n    sagemaker.create_endpoint_config(\n        EndpointConfigName=endpoint_config_name,\n        ProductionVariants=[\n            {\n                \"ModelName\": model_name,\n                \"InstanceType\": \"ml.m5.xlarge\",\n                \"InitialVariantWeight\": 1,\n                \"InitialInstanceCount\": 1,\n                \"VariantName\": \"AllTraffic\",\n            }\n        ],\n        # We can enable Data Capture to record the inputs and outputs\n        # of the endpoint to use them later for monitoring the model.\n        DataCaptureConfig={\n            \"EnableCapture\": True,\n            \"InitialSamplingPercentage\": data_capture_percentage,\n            \"DestinationS3Uri\": data_capture_destination,\n            \"CaptureOptions\": [\n                {\"CaptureMode\": \"Input\"},\n                {\"CaptureMode\": \"Output\"},\n            ],\n            \"CaptureContentTypeHeader\": {\n                \"CsvContentTypes\": [\"text/csv\", \"application/octect-stream\"],\n                \"JsonContentTypes\": [\"application/json\", \"application/octect-stream\"],\n            },\n        },\n    )\n\n    response = sagemaker.list_endpoints(NameContains=endpoint_name, MaxResults=1)\n\n    if len(response[\"Endpoints\"]) == 0:\n        # If the endpoint doesn't exist, let's create it.\n        sagemaker.create_endpoint(\n            EndpointName=endpoint_name,\n            EndpointConfigName=endpoint_config_name,\n        )\n    else:\n        # If the endpoint already exists, let's update it with the\n        # new configuration.\n        sagemaker.update_endpoint(\n            EndpointName=endpoint_name,\n            EndpointConfigName=endpoint_config_name,\n        )\n\n    return {\"statusCode\": 200, \"body\": json.dumps(\"Endpoint deployed successfully\")}\n\n\n\n\nStep 3 - Setting up Lambda Permissions\nWe need to ensure our Lambda function has permission to interact with SageMaker, so let’s create a new role with the appropriate permissions.\n\nlambda_role_name = \"lambda-deployment-role\"\nlambda_role_arn = None\n\ntry:\n    response = iam_client.create_role(\n        RoleName=lambda_role_name,\n        AssumeRolePolicyDocument=json.dumps(\n            {\n                \"Version\": \"2012-10-17\",\n                \"Statement\": [\n                    {\n                        \"Effect\": \"Allow\",\n                        \"Principal\": {\n                            \"Service\": [\"lambda.amazonaws.com\", \"events.amazonaws.com\"],\n                        },\n                        \"Acti,on\": \"sts:AssumeRole\",\n                    },\n                ],\n            },\n        ),\n        Description=\"Lambda Endpoint Deployment\",\n    )\n\n    lambda_role_arn = response[\"Role\"][\"Arn\"]\n\n    iam_client.attach_role_policy(\n        PolicyArn=\"arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole\",\n        RoleName=lambda_role_name,\n    )\n\n    iam_client.attach_role_policy(\n        PolicyArn=\"arn:aws:iam::aws:policy/AmazonSageMakerFullAccess\",\n        RoleName=lambda_role_name,\n    )\n\n    print(f'Role \"{lambda_role_name}\" created with ARN \"{lambda_role_arn}\".')\nexcept iam_client.exceptions.EntityAlreadyExistsException:\n    response = iam_client.get_role(RoleName=lambda_role_name)\n    lambda_role_arn = response[\"Role\"][\"Arn\"]\n    print(f'Role \"{lambda_role_name}\" already exists with ARN \"{lambda_role_arn}\".')\n\n\n\nStep 4 - Creating the Lambda Function\nLet’s now create the Lambda function in AWS. We’ll pass the configuration settings we defined before as environment variables to the Lambda function.\n\nfrom sagemaker.lambda_helper import Lambda\n\ndeploy_lambda_fn = Lambda(\n    function_name=\"deployment_fn\",\n    execution_role_arn=lambda_role_arn,\n    script=(CODE_FOLDER / \"lambda\" / \"lambda.py\").as_posix(),\n    handler=\"lambda.lambda_handler\",\n    timeout=600,\n    session=sagemaker_session,\n    runtime=\"python3.11\",\n    environment={\n        \"Variables\": {\n            \"ENDPOINT\": ENDPOINT,\n            \"DATA_CAPTURE_DESTINATION\": DATA_CAPTURE_DESTINATION,\n            \"DATA_CAPTURE_PERCENTAGE\": str(DATA_CAPTURE_PERCENTAGE),\n            \"ROLE\": role,\n        },\n    },\n)\n\ndeploy_lambda_fn_response = deploy_lambda_fn.upsert()\ndeploy_lambda_fn_response\n\n\n\nStep 5 - Setting up the Lambda Step\nWe can now define the Lambda Step that will run the function to deploy the model. We’ll do this in a function that we can reuse later.\nThis step will send the model package ARN we want to deploy to the Lambda function as an input parameter.\n\nfrom sagemaker.workflow.lambda_step import LambdaStep\n\n\ndef create_deployment_step(register_model_step):\n    \"\"\"Create a Deploy Step using the supplied parameters.\"\"\"\n    return LambdaStep(\n        name=\"deploy\",\n        lambda_func=deploy_lambda_fn,\n        inputs={\n            \"model_package_arn\": register_model_step.properties.ModelPackageArn,\n        },\n    )\n\n\ndeploy_step = create_deployment_step(register_model_step)\n\n\n\nStep 6 - Modifying the Condition Step\nWe need to modify the Condition Step to include the new deployment step. If the condition succeeds, we will register and deploy the model.\n\ncondition_step = ConditionStep(\n    name=\"check-model-accuracy\",\n    conditions=[condition],\n    if_steps=[register_model_step, deploy_step],\n    else_steps=[fail_step],\n)\n\n\n\nStep 7 - Creating the Pipeline\nWe can now define the SageMaker Pipeline and submit its definition to the SageMaker Pipelines service to create the pipeline if it doesn’t exist or update it if it does.\n\nsession12_pipeline = Pipeline(\n    name=\"session12-pipeline\",\n    parameters=[dataset_location, accuracy_threshold],\n    steps=[\n        preprocessing_step,\n        train_model_step,\n        evaluate_model_step,\n        condition_step,\n    ],\n    pipeline_definition_config=pipeline_definition_config,\n    sagemaker_session=config[\"session\"],\n)\n\nsession12_pipeline.upsert(role_arn=role)\n\n\n\nStep 8 - Testing the Endpoint\nLet’s test the endpoint to make sure it works.\nThe wait_for_endpoint function will wait until the endpoint is ready to receive requests.\n\ndef wait_for_endpoint():\n    \"\"\"Wait for the endpoint to come in service.\"\"\"\n    waiter = sagemaker_client.get_waiter(\"endpoint_in_service\")\n    waiter.wait(EndpointName=ENDPOINT, WaiterConfig={\"Delay\": 10, \"MaxAttempts\": 30})\n\n\npayload = \"0.6569590202313976,-1.0813829646495108,1.2097102831892812,0.9226343641317372,1.0,0.0,0.0\"  # noqa: E501\n\n\ntry:\n    wait_for_endpoint()\n\n    predictor = Predictor(endpoint_name=ENDPOINT)\n\n    response = predictor.predict(payload, initial_args={\"ContentType\": \"text/csv\"})\n    response = json.loads(response.decode(\"utf-8\"))\n\n    print(json.dumps(response, indent=2))\nexcept Exception as e:\n    print(e)\n\nWaiter EndpointInService failed: Waiter encountered a terminal failure state: Matched expected service error code: ValidationException",
    "crumbs": [
      "Building Machine Learning Systems That Don't Suck"
    ]
  },
  {
    "objectID": "cohort.html#session-13---deploying-from-an-event",
    "href": "cohort.html#session-13---deploying-from-an-event",
    "title": "Building Machine Learning Systems That Don’t Suck",
    "section": "Session 13 - Deploying From an Event",
    "text": "Session 13 - Deploying From an Event\nThis session modifies the SageMaker Pipeline to register the model with PendingManualApproval status and deploys it whenever its status changes to Approved.\n \nWe will use Amazon EventBridge to trigger a Lambda function that will deploy the model whenever its status changes from “PendingManualApproval” to “Approved.”\n\nStep 1 - Configuring the Model Package Group\nWe need to define the name of a new group where we’ll register models with PendingManualApproval status.\n\nPENDING_MODEL_PACKAGE_GROUP = \"pending-penguins\"\n\n\n\nStep 2 - Setting Up EventBridge\nWe can now create an EventBridge rule that triggers the deployment process whenever a model approval status becomes Approved. To do this, let’s define the event pattern that will trigger the deployment process. Check Model package state change for more information.\n\nevent_pattern = f\"\"\"\n{{\n  \"source\": [\"aws.sagemaker\"],\n  \"detail-type\": [\"SageMaker Model Package State Change\"],\n  \"detail\": {{\n    \"ModelPackageGroupName\": [\"{PENDING_MODEL_PACKAGE_GROUP}\"],\n    \"ModelApprovalStatus\": [\"Approved\"]\n  }}\n}}\n\"\"\"\n\nLet’s now create the EventBridge rule:\n\nrule_name = \"PendingModelApprovedRule\"\n\nevents_client = boto3.client(\"events\")\nrule_response = events_client.put_rule(\n    Name=rule_name,\n    EventPattern=event_pattern,\n    State=\"ENABLED\",\n    RoleArn=role,\n)\n\nNow, we need to define the target of the rule. The target will trigger whenever the rule matches an event. In this case, we want to trigger the Lambda function we created before:\n\nresponse = events_client.put_targets(\n    Rule=rule_name,\n    Targets=[\n        {\n            \"Id\": \"1\",\n            \"Arn\": deploy_lambda_fn_response[\"FunctionArn\"],\n        },\n    ],\n)\n\n\n\nStep 3 - Configuring the Lambda Permissions\nFinally, we need to give the Lambda function permissions to be triggered by the EventBridge rule:\n\nlambda_function_name = deploy_lambda_fn_response[\"FunctionName\"]\nlambda_client = boto3.client(\"lambda\")\n\ntry:\n    response = lambda_client.add_permission(\n        Action=\"lambda:InvokeFunction\",\n        FunctionName=lambda_function_name,\n        Principal=\"events.amazonaws.com\",\n        SourceArn=rule_response[\"RuleArn\"],\n        StatementId=\"EventBridge\",\n    )\nexcept lambda_client.exceptions.ResourceConflictException:\n    print(f'Function \"{lambda_function_name}\" already has the specified permission.')\n\nFunction \"deployment_fn\" already has the specified permission.\n\n\n\n\nStep 4 - Registering the Model\nWe need to modify the Model Step to register the model using PendingManualApproval status.\n\nregister_model_step = create_registration_step(\n    tensorflow_model,\n    PENDING_MODEL_PACKAGE_GROUP,\n    approval_status=\"PendingManualApproval\",\n    model_metrics=model_metrics,\n)\n\n\n\nStep 5 - Modifying the Condition Step\nLet’s modify the Condition Step to include the new registration step. If the condition succeeds, we will register the model with PendingManualApproval status.\n\ncondition_step = ConditionStep(\n    name=\"check-model-accuracy\",\n    conditions=[condition],\n    if_steps=[register_model_step],\n    else_steps=[fail_step],\n)\n\n\n\nStep 6 - Creating the Pipeline\nLet’s define the SageMaker Pipeline and submit its definition to the SageMaker Pipelines service to create the pipeline if it doesn’t exist or update it if it does.\n\nsession13_pipeline = Pipeline(\n    name=\"session13-pipeline\",\n    parameters=[dataset_location, accuracy_threshold],\n    steps=[\n        preprocessing_step,\n        train_model_step,\n        evaluate_model_step,\n        condition_step,\n    ],\n    pipeline_definition_config=pipeline_definition_config,\n    sagemaker_session=config[\"session\"],\n)\n\nsession13_pipeline.upsert(role_arn=role)",
    "crumbs": [
      "Building Machine Learning Systems That Don't Suck"
    ]
  },
  {
    "objectID": "cohort.html#session-14---building-an-inference-pipeline",
    "href": "cohort.html#session-14---building-an-inference-pipeline",
    "title": "Building Machine Learning Systems That Don’t Suck",
    "section": "Session 14 - Building an Inference Pipeline",
    "text": "Session 14 - Building an Inference Pipeline\nThis session creates an inference pipeline to control the data that goes in and comes out of the endpoint.\nDeploying the model we trained directly to an endpoint doesn’t lets us control the data that goes in and comes out of the endpoint. The TensorFlow model we trained requires transformed data, which makes it useless to other applications:\n \nTo fix this, we can create an Inference Pipeline using SageMaker to control the data that goes in and comes out of the endpoint.\nOur inference pipeline will have three components:\n\nA preprocessing component that will transform the input data into the format the model expects.\nThe TensorFlow model.\nA postprocessing component that will transform the output of the model into a human-readable format.\n\n \nWe want our endpoint to handle unprocessed data in CSV and JSON format and return the penguin’s species. Here is an example of the payload input we want the endpoint to support:\n{\n    \"island\": \"Biscoe\",\n    \"culmen_length_mm\": 48.6,\n    \"culmen_depth_mm\": 16.0,\n    \"flipper_length_mm\": 230.0,\n    \"body_mass_g\": 5800.0\n}\nAnd here is an example of the output we’d like to get from the endpoint:\n{\n    \"prediction\": \"Adelie\",\n    \"confidence\": 0.802672\n}\n\nStep 1 - Creating the Preprocessing Script\nThe first component of our inference pipeline will transform the input data into the format the model expects.\nWe’ll use the Scikit-Learn transformer we saved when we split and transformed the data. To deploy this component as part of an inference pipeline, we need to write a script that loads the transformer, uses it to modify the input data, and returns the output in the format the TensorFlow model expects.\nWe’ll store the scripts of every component in a folder called pipeline and add it to the system path so we can later import it as a module.\n\n(CODE_FOLDER / \"pipeline\").mkdir(parents=True, exist_ok=True)\nsys.path.extend([f\"./{CODE_FOLDER}/pipeline\"])\n\nLet’s now create the script for the preprocessing component:\n\n\n\npreprocessing_component.py\n\nimport os\nimport pandas as pd\nimport json\nimport joblib\n\nfrom io import StringIO\n\ntry:\n    from sagemaker_containers.beta.framework import worker\nexcept ImportError:\n    # We don't have access to the `worker` package when testing locally.\n    # We'll set it to None so we can change the way functions create\n    # a response.\n    worker = None\n\n\nTARGET_COLUMN = \"species\"\nFEATURE_COLUMNS = [\n    \"island\",\n    \"culmen_length_mm\",\n    \"culmen_depth_mm\",\n    \"flipper_length_mm\",\n    \"body_mass_g\",\n    \"sex\",\n]\n\n\ndef model_fn(model_dir):\n    \"\"\"\n    Deserializes the model that will be used in this container.\n    \"\"\"\n\n    return joblib.load(os.path.join(model_dir, \"features.joblib\"))\n\n\ndef input_fn(input_data, content_type):\n    \"\"\"\n    Parses the input payload and creates a Pandas DataFrame.\n\n    This function will check whether the target column is present in the\n    input data and will remove it.\n    \"\"\"\n\n    if content_type == \"text/csv\":\n        df = pd.read_csv(StringIO(input_data), header=None, skipinitialspace=True)\n\n        # If we find an extra column, it's probably the target\n        # feature, so let's drop it. We'll assume the target\n        # is always the first column,\n        if len(df.columns) == len(FEATURE_COLUMNS) + 1:\n            df = df.drop(df.columns[0], axis=1)\n\n        df.columns = FEATURE_COLUMNS\n        return df\n\n    if content_type == \"application/json\":\n        df = pd.DataFrame([json.loads(input_data)])\n\n        if TARGET_COLUMN in df.columns:\n            df = df.drop(TARGET_COLUMN, axis=1)\n\n        return df\n\n    raise ValueError(f\"{content_type} is not supported!\")\n\n\ndef predict_fn(input_data, model):\n    \"\"\"\n    Preprocess the input using the transformer.\n    \"\"\"\n\n    try:\n        return model.transform(input_data)\n    except ValueError as e:\n        print(\"Error transforming the input data\", e)\n        return None\n\n\ndef output_fn(prediction, accept):\n    \"\"\"\n    Formats the prediction output to generate a response.\n\n    The default accept/content-type between containers for serial inference\n    is JSON. Since this model preceeds a TensorFlow model, we want to\n    return a JSON object following TensorFlow's input requirements.\n    \"\"\"\n\n    if prediction is None:\n        raise Exception(\"There was an error transforming the input data\")\n\n    instances = [p for p in prediction.tolist()]\n    response = {\"instances\": instances}\n    return (\n        worker.Response(json.dumps(response), mimetype=accept)\n        if worker\n        else (response, accept)\n    )\n\n\nLet’s test the script to ensure everything is working as expected:\n\n\nCode\nfrom pipeline.preprocessing_component import input_fn, predict_fn, output_fn, model_fn\n\n\n@pytest.fixture(scope=\"function\", autouse=False)\ndef directory():\n    directory = tempfile.mkdtemp()\n    input_directory = Path(directory) / \"input\"\n    input_directory.mkdir(parents=True, exist_ok=True)\n    shutil.copy2(DATA_FILEPATH, input_directory / \"data.csv\")\n    \n    directory = Path(directory)\n    \n    preprocess(base_directory=directory)\n    \n    with tarfile.open(directory / \"model\" / \"model.tar.gz\") as tar:\n        tar.extractall(path=directory / \"model\")\n    \n    yield directory / \"model\"\n    \n    shutil.rmtree(directory)\n\n\ndef test_input_csv_drops_target_column_if_present():\n    input_data = \"\"\"\n    Adelie, Torgersen, 39.1, 18.7, 181, 3750, MALE\n    \"\"\"\n    \n    df = input_fn(input_data, \"text/csv\")\n    assert len(df.columns) == 6 and \"species\" not in df.columns\n\n\ndef test_input_json_drops_target_column_if_present():\n    input_data = json.dumps({\n        \"species\": \"Adelie\", \n        \"island\": \"Torgersen\",\n        \"culmen_length_mm\": 44.1,\n        \"culmen_depth_mm\": 18.0,\n        \"flipper_length_mm\": 210.0,\n        \"body_mass_g\": 4000.0,\n        \"sex\": \"MALE\"\n    })\n    \n    df = input_fn(input_data, \"application/json\")\n    assert len(df.columns) == 6 and \"species\" not in df.columns\n\n\ndef test_input_csv_works_without_target_column():\n    input_data = \"\"\"\n    Torgersen, 39.1, 18.7, 181, 3750, MALE\n    \"\"\"\n    \n    df = input_fn(input_data, \"text/csv\")\n    assert len(df.columns) == 6\n\n\ndef test_input_json_works_without_target_column():\n    input_data = json.dumps({\n        \"island\": \"Torgersen\",\n        \"culmen_length_mm\": 44.1,\n        \"culmen_depth_mm\": 18.0,\n        \"flipper_length_mm\": 210.0,\n        \"body_mass_g\": 4000.0,\n        \"sex\": \"MALE\"\n    })\n    \n    df = input_fn(input_data, \"application/json\")\n    assert len(df.columns) == 6\n\n\ndef test_output_raises_exception_if_prediction_is_none():\n    with pytest.raises(Exception):\n        output_fn(None, \"application/json\")\n    \n    \ndef test_output_returns_tensorflow_ready_input():\n    prediction = np.array([\n        [-1.3944109908736013,1.15488062669371,-0.7954340636549508,-0.5536447804097907,0.0,1.0,0.0],\n        [1.0557485835338234,0.5040085971987002,-0.5824506029515057,-0.5851840035995248,0.0,1.0,0.0]\n    ])\n    \n    response = output_fn(prediction, \"application/json\")\n    \n    assert response[0] == {\n        \"instances\": [\n            [-1.3944109908736013,1.15488062669371,-0.7954340636549508,-0.5536447804097907,0.0,1.0,0.0],\n            [1.0557485835338234,0.5040085971987002,-0.5824506029515057,-0.5851840035995248,0.0,1.0,0.0]\n        ]\n    }\n    \n    assert response[1] == \"application/json\"\n\n    \ndef test_predict_transforms_data(directory):\n    input_data = \"\"\"\n    Torgersen, 39.1, 18.7, 181, 3750, MALE\n    \"\"\"\n    \n    model = model_fn(directory.as_posix())\n    df = input_fn(input_data, \"text/csv\")\n    response = predict_fn(df, model)\n    assert type(response) is np.ndarray\n    \n\ndef test_predict_returns_none_if_invalid_input(directory):\n    input_data = \"\"\"\n    Invalid, 39.1, 18.7, 181, 3750, MALE\n    \"\"\"\n    \n    model = model_fn(directory.as_posix())\n    df = input_fn(input_data, \"text/csv\")\n    assert predict_fn(df, model) is None\n\n\n\n\nStep 2 - Creating the Postprocessing Script\nThe final component of our inference pipeline will transform the output from the model into a human-readable format.\nWe’ll use the Scikit-Learn target transformer we saved when we split and transformed the data. To deploy this component as part of an inference pipeline, we need to write a script that loads the transformer, uses it to modify the output from the model, and returns a human-readable format.\n\n\n\npostprocessing_component.py\n\nimport os\nimport numpy as np\nimport json\nimport joblib\n\n\ntry:\n    from sagemaker_containers.beta.framework import encoders, worker\nexcept ImportError:\n    # We don't have access to the `worker` package when testing locally.\n    # We'll set it to None so we can change the way functions create\n    # a response.\n    worker = None\n\n\ndef model_fn(model_dir):\n    \"\"\"\n    Deserializes the target model and returns the list of fitted categories.\n    \"\"\"\n\n    model = joblib.load(os.path.join(model_dir, \"target.joblib\"))\n    return model.named_transformers_[\"species\"].categories_[0]\n\n\ndef input_fn(input_data, content_type):\n    if content_type == \"application/json\":\n        return json.loads(input_data)[\"predictions\"]\n    \n    raise ValueError(f\"{content_type} is not supported.\")\n\n\ndef predict_fn(input_data, model):\n    \"\"\"\n    Transforms the prediction into its corresponding category.\n    \"\"\"\n    predictions = np.argmax(input_data, axis=-1)\n    confidence = np.max(input_data, axis=-1)\n    return [\n        (model[prediction], confidence)\n        for confidence, prediction in zip(confidence, predictions)\n    ]\n\ndef output_fn(prediction, accept):\n    if accept == \"text/csv\":\n        return (\n            worker.Response(encoders.encode(prediction, accept), mimetype=accept)\n            if worker\n            else (prediction, accept)\n        )\n\n    if accept == \"application/json\":\n        response = []\n        for p, c in prediction:\n            response.append({\"prediction\": p, \"confidence\": c})\n\n        # If there's only one prediction, we'll return it\n        # as a single object.\n        if len(response) == 1:\n            response = response[0]\n\n        return (\n            worker.Response(json.dumps(response), mimetype=accept)\n            if worker\n            else (response, accept)\n        )\n\n    raise Exception(f\"{accept} accept type is not supported.\")\n\n\nLet’s test the script to ensure everything is working as expected:\n\n\nCode\nimport numpy as np\n\nfrom pipeline.postprocessing_component import predict_fn, output_fn\n\n\ndef test_predict_returns_prediction_as_first_column():\n    input_data = [\n        [0.6, 0.2, 0.2], \n        [0.1, 0.8, 0.1],\n        [0.2, 0.1, 0.7]\n    ]\n    \n    categories = [\"Adelie\", \"Gentoo\", \"Chinstrap\"]\n    \n    response = predict_fn(input_data, categories)\n    \n    assert response == [\n        (\"Adelie\", 0.6),\n        (\"Gentoo\", 0.8),\n        (\"Chinstrap\", 0.7)\n    ]\n\n\ndef test_output_does_not_return_array_if_single_prediction():\n    prediction = [(\"Adelie\", 0.6)]\n    response, _ = output_fn(prediction, \"application/json\")\n\n    assert response[\"prediction\"] == \"Adelie\"\n\n\ndef test_output_returns_array_if_multiple_predictions():\n    prediction = [(\"Adelie\", 0.6), (\"Gentoo\", 0.8)]\n    response, _ = output_fn(prediction, \"application/json\")\n\n    assert len(response) == 2\n    assert response[0][\"prediction\"] == \"Adelie\"\n    assert response[1][\"prediction\"] == \"Gentoo\"\n\n\n\n\nStep 3 - Setting up the Inference Pipeline\nWe can now create a PipelineModel to define our inference pipeline.\nWe’ll use the model we generated in the Split and Transform step as the input to the first and last components of the inference pipeline. This model.tar.gz file contains the two transformers we need to preprocess and postprocess the data.\nLet’s create a variable with the URI to this file:\n\ntransformation_pipeline_model = Join(\n    on=\"/\",\n    values=[\n        preprocessing_step.properties.ProcessingOutputConfig.Outputs[\n            \"model\"\n        ].S3Output.S3Uri,\n        \"model.tar.gz\",\n    ],\n)\n\nHere is the first component of the inference pipeline. It will preprocess the data before sending it to the TensorFlow model:\n\nfrom sagemaker.sklearn.model import SKLearnModel\n\npreprocessing_model = SKLearnModel(\n    model_data=transformation_pipeline_model,\n    entry_point=\"preprocessing_component.py\",\n    source_dir=(CODE_FOLDER / \"pipeline\").as_posix(),\n    framework_version=\"1.2-1\",\n    sagemaker_session=config[\"session\"],\n    role=role,\n)\n\nHere is the last component of the inference pipeline. It will postprocess the output from the TensorFlow model before sending it back to the user:\n\npostprocessing_model = SKLearnModel(\n    model_data=transformation_pipeline_model,\n    entry_point=\"postprocessing_component.py\",\n    source_dir=(CODE_FOLDER / \"pipeline\").as_posix(),\n    framework_version=\"1.2-1\",\n    sagemaker_session=config[\"session\"],\n    role=role,\n)\n\nWe can now create the inference pipeline using the three models:\n\nfrom sagemaker.pipeline import PipelineModel\n\npipeline_model = PipelineModel(\n    name=\"inference-model\",\n    models=[preprocessing_model, tensorflow_model, postprocessing_model],\n    sagemaker_session=config[\"session\"],\n    role=role,\n)\n\n\n\nStep 4 - Configuring the Model Package Group\nLet’s define a new package group to register the Pipeline Model:\n\nPIPELINE_MODEL_PACKAGE_GROUP = \"pipeline-penguins\"\n\n\n\nStep 5 - Registering the Model\nWe’ll modify the registration step to register the Pipeline Model in the Model Registry.\n\nregister_model_step = create_registration_step(\n    pipeline_model,\n    PIPELINE_MODEL_PACKAGE_GROUP,\n    content_types=[\"text/csv\", \"application/json\"],\n    response_types=[\"text/csv\", \"application/json\"],\n    model_metrics=model_metrics,\n)\n\n\n\nStep 6 - Modifying the Deploy Step\nLet’s now modify the LambdaStep to use the updated Registration Step.\n\ndeploy_step = create_deployment_step(register_model_step)\n\n\n\nStep 7 - Modifying the Condition Step\nSince we modified the Registration Step, we also need to modify the Condition Step to use the new registration:\n\ncondition_step = ConditionStep(\n    name=\"check-model-accuracy\",\n    conditions=[condition],\n    if_steps=[register_model_step, deploy_step],\n    else_steps=[fail_step],\n)\n\n\n\nStep 8 - Creating the Pipeline\nWe can now define the SageMaker Pipeline and submit its definition to the SageMaker Pipelines service to create the pipeline if it doesn’t exist or update it if it does.\n\nsession14_pipeline = Pipeline(\n    name=\"session14-pipeline\",\n    parameters=[dataset_location, accuracy_threshold],\n    steps=[\n        preprocessing_step,\n        train_model_step,\n        evaluate_model_step,\n        condition_step,\n    ],\n    pipeline_definition_config=pipeline_definition_config,\n    sagemaker_session=config[\"session\"],\n)\n\nsession14_pipeline.upsert(role_arn=role)\n\n\n\nStep 9 - Testing the Endpoint\nLet’s now test the endpoint. Notice that we can now send the raw data to the endpoint, and it will return the penguin’s species in a human-readable format.\n\nfrom sagemaker.serializers import CSVSerializer\n\npredictor = Predictor(\n    endpoint_name=ENDPOINT,\n    serializer=CSVSerializer(),\n    sagemaker_session=sagemaker_session,\n)\n\ndata = pd.read_csv(DATA_FILEPATH)\ndata = data.drop(\"species\", axis=1)\n\npayload = data.iloc[:3].to_csv(header=False, index=False)\nprint(f\"Payload:\\n{payload}\")\n\ntry:\n    wait_for_endpoint()\n\n    response = predictor.predict(payload, initial_args={\"ContentType\": \"text/csv\"})\n    response = json.loads(response.decode(\"utf-8\"))\n    print(json.dumps(response, indent=2))\nexcept Exception as e:\n    print(e)\n\nPayload:\nTorgersen,39.1,18.7,181.0,3750.0,MALE\nTorgersen,39.5,17.4,186.0,3800.0,FEMALE\nTorgersen,40.3,18.0,195.0,3250.0,FEMALE\n\nWaiter EndpointInService failed: Waiter encountered a terminal failure state: Matched expected service error code: ValidationException\n\n\nWe can also test the endpoint by sending a JSON payload. Notice how you can use a deserealizer to automatically decode the response from the model.\n\nfrom sagemaker.deserializers import JSONDeserializer\nfrom sagemaker.serializers import JSONSerializer\n\nsample = {\n    \"island\": \"Biscoe\",\n    \"culmen_length_mm\": 48.6,\n    \"culmen_depth_mm\": 16.0,\n    \"flipper_length_mm\": 230.0,\n    \"body_mass_g\": 5800.0,\n    \"sex\": \"MALE\",\n}\n\npredictor = Predictor(\n    endpoint_name=ENDPOINT,\n    serializer=JSONSerializer(),\n    deserializer=JSONDeserializer(),\n    sagemaker_session=sagemaker_session,\n)\n\ntry:\n    response = predictor.predict(sample)\n    print(response)\nexcept Exception as e:\n    print(e)\n\nAn error occurred (ValidationError) when calling the InvokeEndpoint operation: Endpoint penguins-endpoint of account 325223348818 not found.\n\n\nAnd now let’s send the same payload but return the prediction in CSV format:\n\nfrom sagemaker.deserializers import CSVDeserializer\n\npredictor = Predictor(\n    endpoint_name=ENDPOINT,\n    serializer=JSONSerializer(),\n    deserializer=CSVDeserializer(),\n    sagemaker_session=sagemaker_session,\n)\n\ntry:\n    response = predictor.predict(sample, initial_args={\"Accept\": \"text/csv\"})\n    print(response)\nexcept Exception as e:\n    print(e)\n\nAn error occurred (ValidationError) when calling the InvokeEndpoint operation: Endpoint penguins-endpoint of account 325223348818 not found.",
    "crumbs": [
      "Building Machine Learning Systems That Don't Suck"
    ]
  },
  {
    "objectID": "cohort.html#session-15---custom-inference-script",
    "href": "cohort.html#session-15---custom-inference-script",
    "title": "Building Machine Learning Systems That Don’t Suck",
    "section": "Session 15 - Custom Inference Script",
    "text": "Session 15 - Custom Inference Script\nThis session creates a custom inference script to control the inference process in the SageMaker endpoint. This is an alternative to creating an inference pipeline to preprocess and postprocess the data that comes in and out of the model.\n\nStep 1 - Creating the Inference Script\nLet’s create a script where we’ll manage the inference process in the endpoint.\nWe’ll’ include this code as part of the model assets to control the inference process on the SageMaker endpoint. SageMaker will automatically call the handler() function for every request to the endpoint. Check How to implement the pre- and/or post-processing handler(s) for more information.\nWe can now create the script inside the folder.\n\n\n\ninference.py\n\nimport os\nimport json\nimport requests\nimport joblib\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\n\ndef handler(data, context, directory=Path(\"/opt/ml/model\")):\n    \"\"\"\n    This is the entrypoint that will be called by SageMaker\n    when the endpoint receives a request.\n    \"\"\"\n    print(\"Handling endpoint request\")\n\n    processed_input = _process_input(data, context, directory)\n    output = _predict(processed_input, context, directory) if processed_input else None\n    return _process_output(output, context, directory)\n\n\ndef _process_input(data, context, directory):\n    print(\"Processing input data...\")\n\n    if context is None:\n        # The context will be None when we are testing the code\n        # directly from a notebook. In that case, we can use the\n        # data directly.\n        endpoint_input = data\n    elif context.request_content_type in (\n        \"application/json\",\n        \"application/octet-stream\",\n    ):\n        # When the endpoint is running, we will receive a context\n        # object. We need to parse the input and turn it into\n        # JSON in that case.\n        endpoint_input = data.read().decode(\"utf-8\")\n    else:\n        raise ValueError(\n            f\"Unsupported content type: {context.request_content_type or 'unknown'}\"\n        )\n\n    # Let's now transform the input data using the features pipeline.\n    try:\n        endpoint_input = json.loads(endpoint_input)\n        df = pd.json_normalize(endpoint_input)\n        features_pipeline = joblib.load(directory / \"features.joblib\")\n        result = features_pipeline.transform(df)\n    except Exception as e:\n        print(f\"There was an error processing the input data. {e}\")\n        return None\n\n    return result[0].tolist()\n\n\ndef _predict(instance, context, directory):\n    print(\"Sending input data to model to make a prediction...\")\n\n    if context is None:\n        # The context will be None when we are testing the code\n        # directly from a notebook. In that case, we want to load the\n        # model we trained and make a prediction using it.\n        import keras\n\n        model = keras.models.load_model(Path(directory) / \"001\")\n        predictions = model.predict(np.array([instance]))\n        result = {\"predictions\": predictions.tolist()}\n    else:\n        # When the endpoint is running, we will receive a context\n        # object. In that case we need to send the instance to the\n        # model to get a prediction back.\n        model_input = json.dumps({\"instances\": [instance]})\n        response = requests.post(context.rest_uri, data=model_input)\n\n        if response.status_code != 200:\n            raise ValueError(response.content.decode(\"utf-8\"))\n\n        result = json.loads(response.content)\n\n    print(f\"Response: {result}\")\n    return result\n\n\ndef _process_output(output, context, directory):\n    print(\"Processing prediction received from the model...\")\n\n    if output:\n        prediction = np.argmax(output[\"predictions\"][0])\n        confidence = output[\"predictions\"][0][prediction]\n\n        target_pipeline = joblib.load(directory / \"target.joblib\")\n        classes = target_pipeline.named_transformers_[\"species\"].categories_[0]\n\n        result = {\n            \"prediction\": classes[prediction],\n            \"confidence\": confidence,\n        }\n    else:\n        result = {\"prediction\": None}\n\n    print(result)\n\n    response_content_type = (\n        \"application/json\" if context is None else context.accept_header\n    )\n    return json.dumps(result), response_content_type\n\n\nLet’s test the script to ensure everything is working as expected:\n\n\nCode\nimport os\nimport shutil\nimport tarfile\nimport pytest\nimport tempfile\n\nfrom processing.script import preprocess\nfrom training.script import train\nfrom pipeline.inference import handler\n\n\n@pytest.fixture(scope=\"function\", autouse=False)\ndef directory():\n    directory = tempfile.mkdtemp()\n    input_directory = Path(directory) / \"input\"\n    input_directory.mkdir(parents=True, exist_ok=True)\n    shutil.copy2(DATA_FILEPATH, input_directory / \"data.csv\")\n\n    directory = Path(directory)\n\n    preprocess(base_directory=directory)\n\n    train(\n        model_directory=directory / \"model\",\n        train_path=directory / \"train\",\n        validation_path=directory / \"validation\",\n        pipeline_path=directory / \"model\",\n        experiment=None,\n        epochs=1,\n    )\n\n    # After training a model, we need to prepare a package just like\n    # SageMaker would. This package is what the evaluation script is\n    # expecting as an input.\n    with tarfile.open(directory / \"model.tar.gz\", \"w:gz\") as tar:\n        tar.add(directory / \"model\" / \"001\", arcname=\"001\")\n\n    yield directory\n\n    shutil.rmtree(directory)\n\n\n@pytest.fixture(scope=\"function\", autouse=False)\ndef payload():\n    return json.dumps({\n        \"island\": \"Biscoe\",\n        \"culmen_length_mm\": 48.6,\n        \"culmen_depth_mm\": 16.0,\n        \"flipper_length_mm\": 230.0,\n        \"body_mass_g\": 5800,\n    }).encode(\"utf-8\")\n\n\ndef test_handler_response_contains_prediction_and_confidence(directory, payload):\n    response = handler(\n        data=payload,\n        context=None,\n        directory=directory / \"model\",\n    )\n\n    response = json.loads(response[0])\n    assert \"prediction\" in response\n    assert \"confidence\" in response\n\n\ndef test_handler_response_includes_content_type(directory, payload):\n    response = handler(\n        data=payload,\n        context=None,\n        directory=directory / \"model\",\n    )\n\n    assert response[1] == \"application/json\"\n\n\ndef test_handler_response_prediction_is_categorical(directory, payload):\n    response = handler(\n        data=payload,\n        context=None,\n        directory=directory / \"model\",\n    )\n\n    response = json.loads(response[0])\n    assert response[\"prediction\"] in [\"Adelie\", \"Gentoo\", \"Chinstrap\"]\n\n\ndef test_handler_deals_with_an_invalid_payload(directory):\n    response = handler(\n        data=\"invalid payload\",\n        context=None,\n        directory=directory / \"model\",\n    )\n\n    response = json.loads(response[0])\n    assert response[\"prediction\"] is None\n\n\n\n\nStep 2 - Creating the Model\nWe can now create a new TensorFlowModel including the inference.py file.\nSageMaker triggers a repack operation whenever we specify the source_dir attribute in a model. We want that attribute to point to the local folder containing the inference.py file. SageMaker will automatically modify the original model.tar.gz package to include a /code folder containing the file.\nSince we need access to Scikit-Learn in our script, we can include a requirements.txt file in the same location where the inference.py script is, and SageMaker will install everything in it.\nTo repack the model assets, SageMaker will automatically include a new step in the pipeline right before registering the model.\nHere is what the new model.tar.gz package will look like:\nmodel/\n    |--[model_version_number]\n        |--assets/\n        |--variables/\n        |--saved_model.pb\n    |--features.joblib\n    |--target.joblib\ncode/\n    |--inference.py\n    |--requirements.txt\nLet’s create a requirements.txt file with all the libraries we want SageMaker to install in the inference container.\n\n\n\nrequirements.txt\n\nnumpy\npandas\nscikit-learn==1.2.1\n\n\nWe can now create the model using the inference.py script.\n\ncustom_tensorflow_model = TensorFlowModel(\n    name=\"penguins\",\n    model_data=train_model_step.properties.ModelArtifacts.S3ModelArtifacts,\n    entry_point=\"inference.py\",\n    source_dir=(CODE_FOLDER / \"pipeline\").as_posix(),\n    framework_version=config[\"framework_version\"],\n    sagemaker_session=config[\"session\"],\n    role=role,\n)\n\n\n\nStep 3 - Configuring the Model Package Group\nLet’s define a new group where we’ll register the model using the custom inference.py script.\n\nCUSTOM_MODEL_PACKAGE_GROUP = \"custom-penguins\"\n\n\n\nStep 4 - Registering the Model\nWe can now modify the registration step to register the custom model in the Model Registry.\n\nregister_model_step = create_registration_step(\n    custom_tensorflow_model,\n    model_package_group_name=CUSTOM_MODEL_PACKAGE_GROUP,\n    content_types=[\"application/json\"],\n    response_types=[\"application/json\"],\n    model_metrics=model_metrics,\n)\n\n\n\nStep 5 - Modifying the Deploy Step\nLet’s now modify the LambdaStep to use the updated Registration Step.\n\ndeploy_step = create_deployment_step(register_model_step)\n\n\n\nStep 6 - Modifying the Condition Step\nSince we modified the Registration Step, we also need to modify the Condition Step to use the new registration:\n\ncondition_step = ConditionStep(\n    name=\"check-model-accuracy\",\n    conditions=[condition],\n    if_steps=[register_model_step, deploy_step],\n    else_steps=[fail_step],\n)\n\n\n\nStep 7 - Creating the Pipeline\nWe can now define the SageMaker Pipeline and submit its definition to the SageMaker Pipelines service to create the pipeline if it doesn’t exist or update it if it does.\n\nsession15_pipeline = Pipeline(\n    name=\"session15-pipeline\",\n    parameters=[dataset_location, accuracy_threshold],\n    steps=[\n        preprocessing_step,\n        train_model_step,\n        evaluate_model_step,\n        condition_step,\n    ],\n    pipeline_definition_config=pipeline_definition_config,\n    sagemaker_session=config[\"session\"],\n)\n\nsession15_pipeline.upsert(role_arn=role)\n\n\n\nStep 8 - Testing the Endpoint\nLet’s test the endpoint to make sure it works.\n\nfrom sagemaker.deserializers import JSONDeserializer\n\ntry:\n    wait_for_endpoint()\n\n    predictor = Predictor(\n        endpoint_name=ENDPOINT,\n        serializer=JSONSerializer(),\n        deserializer=JSONDeserializer(),\n    )\n\n    response = predictor.predict(\n        {\n            \"island\": \"Dream\",\n            \"culmen_length_mm\": 46.4,\n            \"culmen_depth_mm\": 18.6,\n            \"flipper_length_mm\": 190.0,\n            \"body_mass_g\": 3450.0,\n        },\n    )\n\n    print(response)\n\nexcept Exception as e:\n    print(e)\n\nWaiter EndpointInService failed: Waiter encountered a terminal failure state: Matched expected service error code: ValidationException",
    "crumbs": [
      "Building Machine Learning Systems That Don't Suck"
    ]
  },
  {
    "objectID": "cohort.html#session-16---data-quality-baseline",
    "href": "cohort.html#session-16---data-quality-baseline",
    "title": "Building Machine Learning Systems That Don’t Suck",
    "section": "Session 16 - Data Quality Baseline",
    "text": "Session 16 - Data Quality Baseline\nThis session extends the SageMaker Pipeline with a Quality Check Step to compute a baseline for the data the endpoint expects.\nThis step will compute statistics and constraints from the data. We’ll’ later use this information as the baseline to detect data drift and other data quality issues.\n \nCheck Monitor data quality for more information about monitoring data quality in SageMaker.\n\nStep 1 - Configuring Baseline Location\nLet’s start by defining the location where SageMaker will store the baseline data:\n\nDATA_QUALITY_LOCATION = f\"{S3_LOCATION}/monitoring/data-quality\"\n\n\n\nStep 2 - Generating Data Quality Baseline\nLet’s configure a Quality Check Step to compute the general statistics of the data we used to build our model.\nWe can configure the instance that will run the quality check using the CheckJobConfig class, and we can use the DataQualityCheckConfig class to configure the job.\nWe are running this step with the following configuration:\n\nskip_check = True: This parameter controls whether the step should skip checking the data against a previous baseline. Since we want to generate the baseline for the first time, we set it to True. After running the pipeline once to generate the baseline, we can set this parameter to False to ensure any new data follows the same distribution as the baseline.\nregister_new_baseline = True: This parameter controls whether the new calculated baseline will be registered in the Model Registry.\n\nFor more information about these configuration parameters, check Baseline calculation and registration.\n\nfrom sagemaker.model_monitor.dataset_format import DatasetFormat\nfrom sagemaker.workflow.check_job_config import CheckJobConfig\nfrom sagemaker.workflow.quality_check_step import (\n    DataQualityCheckConfig,\n    QualityCheckStep,\n)\n\ndata_quality_baseline_step = QualityCheckStep(\n    name=\"generate-data-quality-baseline\",\n    check_job_config=CheckJobConfig(\n        instance_type=\"ml.c5.xlarge\",\n        instance_count=1,\n        volume_size_in_gb=20,\n        sagemaker_session=config[\"session\"],\n        role=role,\n    ),\n    quality_check_config=DataQualityCheckConfig(\n        baseline_dataset=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\n            \"train-baseline\"\n        ].S3Output.S3Uri,\n        dataset_format=DatasetFormat.csv(header=True),\n        output_s3_uri=DATA_QUALITY_LOCATION,\n    ),\n    model_package_group_name=PIPELINE_MODEL_PACKAGE_GROUP,\n    skip_check=True,\n    register_new_baseline=True,\n    cache_config=cache_config,\n)\n\n\n\nStep 3 - Setting up Model Metrics\nWe can configure a new set of ModelMetrics using the results of the Quality Step. Check Baseline and model version lifecycle and evolution with SageMaker Pipelines for an explanation of how SageMaker uses the DriftCheckBaselines.\n\nfrom sagemaker.drift_check_baselines import DriftCheckBaselines\n\ndata_quality_model_metrics = ModelMetrics(\n    model_data_statistics=MetricsSource(\n        s3_uri=data_quality_baseline_step.properties.CalculatedBaselineStatistics,\n        content_type=\"application/json\",\n    ),\n    model_data_constraints=MetricsSource(\n        s3_uri=data_quality_baseline_step.properties.CalculatedBaselineConstraints,\n        content_type=\"application/json\",\n    ),\n)\n\ndata_quality_drift_check_baselines = DriftCheckBaselines(\n    model_data_statistics=MetricsSource(\n        s3_uri=data_quality_baseline_step.properties.BaselineUsedForDriftCheckStatistics,\n        content_type=\"application/json\",\n    ),\n    model_data_constraints=MetricsSource(\n        s3_uri=data_quality_baseline_step.properties.BaselineUsedForDriftCheckConstraints,\n        content_type=\"application/json\",\n    ),\n)\n\n\n\nStep 4 - Registering the Model\nLet’s modify the registration step to use the new metrics and the drift baseline:\n\nregister_model_step = create_registration_step(\n    pipeline_model,\n    PIPELINE_MODEL_PACKAGE_GROUP,\n    content_types=[\"text/csv\", \"application/json\"],\n    response_types=[\"text/csv\", \"application/json\"],\n    model_metrics=data_quality_model_metrics,\n    drift_check_baselines=data_quality_drift_check_baselines,\n)\n\n\n\nStep 5 - Modifying the Condition Step\nSince we modified the Registration Step, we also need to modify the Condition Step to use the new registration:\n\ncondition_step = ConditionStep(\n    name=\"check-model-accuracy\",\n    conditions=[condition],\n    if_steps=[register_model_step],\n    else_steps=[fail_step],\n)\n\n\n\nStep 6 - Creating the Pipeline\nWe can now define the SageMaker Pipeline and submit its definition to the SageMaker Pipelines service to create the pipeline if it doesn’t exist or update it if it does.\n\nsession16_pipeline = Pipeline(\n    name=\"session16-pipeline\",\n    parameters=[dataset_location, accuracy_threshold],\n    steps=[\n        preprocessing_step,\n        train_model_step,\n        evaluate_model_step,\n        data_quality_baseline_step,\n        condition_step,\n    ],\n    pipeline_definition_config=pipeline_definition_config,\n    sagemaker_session=config[\"session\"],\n)\n\nsession16_pipeline.upsert(role_arn=role)\n\n\n\nStep 7 - Checking Constraints and Statistics\nOur pipeline generated data baseline statistics and constraints. We can take a look at what these values look like by downloading them from S3. You need to wait for the pipeline to finish running before these files are available.\nHere are the data quality statistics:\n\ntry:\n    response = json.loads(\n        S3Downloader.read_file(f\"{DATA_QUALITY_LOCATION}/statistics.json\"),\n    )\n    print(json.dumps(response[\"features\"][0], indent=2))\nexcept Exception:  # noqa: S110\n    pass\n\n{\n  \"name\": \"island\",\n  \"inferred_type\": \"String\",\n  \"string_statistics\": {\n    \"common\": {\n      \"num_present\": 236,\n      \"num_missing\": 0\n    },\n    \"distinct_count\": 3.0,\n    \"distribution\": {\n      \"categorical\": {\n        \"buckets\": [\n          {\n            \"value\": \"Dream\",\n            \"count\": 84\n          },\n          {\n            \"value\": \"Torgersen\",\n            \"count\": 32\n          },\n          {\n            \"value\": \"Biscoe\",\n            \"count\": 120\n          }\n        ]\n      }\n    }\n  }\n}\n\n\nHere are the data quality constraints:\n\ntry:\n    response = json.loads(\n        S3Downloader.read_file(f\"{DATA_QUALITY_LOCATION}/constraints.json\"),\n    )\n    print(json.dumps(response, indent=2))\nexcept Exception:  # noqa: S110\n    pass\n\n{\n  \"version\": 0.0,\n  \"features\": [\n    {\n      \"name\": \"island\",\n      \"inferred_type\": \"String\",\n      \"completeness\": 1.0,\n      \"string_constraints\": {\n        \"domains\": [\n          \"Dream\",\n          \"Torgersen\",\n          \"Biscoe\"\n        ]\n      }\n    },\n    {\n      \"name\": \"culmen_length_mm\",\n      \"inferred_type\": \"Fractional\",\n      \"completeness\": 1.0,\n      \"num_constraints\": {\n        \"is_non_negative\": true\n      }\n    },\n    {\n      \"name\": \"culmen_depth_mm\",\n      \"inferred_type\": \"Fractional\",\n      \"completeness\": 1.0,\n      \"num_constraints\": {\n        \"is_non_negative\": true\n      }\n    },\n    {\n      \"name\": \"flipper_length_mm\",\n      \"inferred_type\": \"Fractional\",\n      \"completeness\": 1.0,\n      \"num_constraints\": {\n        \"is_non_negative\": true\n      }\n    },\n    {\n      \"name\": \"body_mass_g\",\n      \"inferred_type\": \"Fractional\",\n      \"completeness\": 1.0,\n      \"num_constraints\": {\n        \"is_non_negative\": true\n      }\n    },\n    {\n      \"name\": \"sex\",\n      \"inferred_type\": \"String\",\n      \"completeness\": 1.0,\n      \"string_constraints\": {\n        \"domains\": [\n          \"FEMALE\",\n          \".\",\n          \"MALE\"\n        ]\n      }\n    }\n  ],\n  \"monitoring_config\": {\n    \"evaluate_constraints\": \"Enabled\",\n    \"emit_metrics\": \"Enabled\",\n    \"datatype_check_threshold\": 1.0,\n    \"domain_content_threshold\": 1.0,\n    \"distribution_constraints\": {\n      \"perform_comparison\": \"Enabled\",\n      \"comparison_threshold\": 0.1,\n      \"comparison_method\": \"Robust\",\n      \"categorical_comparison_threshold\": 0.1,\n      \"categorical_drift_method\": \"LInfinity\"\n    }\n  }\n}",
    "crumbs": [
      "Building Machine Learning Systems That Don't Suck"
    ]
  },
  {
    "objectID": "cohort.html#session-17---model-quality-baseline",
    "href": "cohort.html#session-17---model-quality-baseline",
    "title": "Building Machine Learning Systems That Don’t Suck",
    "section": "Session 17 - Model Quality Baseline",
    "text": "Session 17 - Model Quality Baseline\nThis session extends the SageMaker Pipeline with a Quality Check Step to compute a baseline for the model performance.\nThis step will compute the baseline metrics we will later use as the baseline to detect model drift.\nTo create a baseline to compare the model performance, we must create predictions for the test set and compare the model’s metrics with the model performance on production data. We can do this by running a Batch Transform Job to predict every sample from the test set. We can use a Transform Step as part of the pipeline to run this job.\n \nCheck Monitor model quality for more information about monitoring model quality in SageMaker.\n\nStep 1 - Configuring Baseline Location\nLet’s start by defining the location where SageMaker will store the baseline data:\n\nMODEL_QUALITY_LOCATION = f\"{S3_LOCATION}/monitoring/model-quality\"\n\n\n\nStep 2 - Creating the Model\nThe Transform Step requires a model to generate predictions, so we need a Model Step that creates a model:\n\ncreate_model_step = ModelStep(\n    name=\"create-model\",\n    step_args=pipeline_model.create(instance_type=config[\"instance_type\"]),\n)\n\n\n\nStep 3 - Setting up the Transform Step\nWe are going to use a Batch Transform Job to generate predictions for every sample from the test set.\nThis Batch Transform Job will run every sample from the training dataset through the model so we can compute the baseline metrics. Check Run a Batch Transform Job for more information about running a Batch Transform Job.\nLet’s start by configuring a Transformer instance:\n\nfrom sagemaker.transformer import Transformer\n\ntransformer = Transformer(\n    model_name=create_model_step.properties.ModelName,\n    instance_type=config[\"instance_type\"],\n    instance_count=1,\n    strategy=\"MultiRecord\",\n    accept=\"text/csv\",\n    assemble_with=\"Line\",\n    output_path=f\"{S3_LOCATION}/transform\",\n    sagemaker_session=config[\"session\"],\n)\n\nWe can now set up the Transform Step using the Transformer we configured before.\nNotice the following:\n\nWe’ll generate predictions for the baseline test data that we generated when we split and transformed the data. This baseline is the same data we used to test the model, but it’s in raw format.\nThe output of this Batch Transform Job will have two fields. The first one will be the ground truth label, and the second one will be the prediction of the model.\n\n\nfrom sagemaker.workflow.steps import TransformStep\n\ngenerate_test_predictions_step = TransformStep(\n    name=\"generate-test-predictions\",\n    step_args=transformer.transform(\n        # We will use the baseline set we generated when we split the data.\n        # This set corresponds to the test split before the transformation step.\n        data=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\n            \"test-baseline\"\n        ].S3Output.S3Uri,\n        join_source=\"Input\",\n        split_type=\"Line\",\n        content_type=\"text/csv\",\n        # We want to output the first and the second to last field from\n        # the joint set. The first field corresponds to the groundtruth,\n        # and the second to last field corresponds to the prediction.\n        #\n        # Here is an example of the data the Transform Job will generate\n        # after joining the input with the output from the model:\n        #\n        # Gentoo,39.1,18.7,181.0,3750.0,MALE,Gentoo,0.52\n        #\n        # Notice how the first field is the groundtruth coming from the\n        # test set. The second to last field is the prediction coming the\n        # model.\n        output_filter=\"$[0,-2]\",\n    ),\n    cache_config=cache_config,\n)\n\n\n\nStep 4 - Generating Model Quality Baseline\nLet’s now configure the Quality Check Step and feed it the data we generated in the Transform Step. This step will automatically compute the performance metrics of the model on the test set.\nWe are running this step with the following configuration:\n\nskip_check = True: This parameter controls whether the step should skip checking the data against a previous baseline. Since we want to generate the baseline for the first time, we set it to True. After running the pipeline once to generate the baseline, we can set this parameter to False to ensure any new data follows the same distribution as the baseline.\nregister_new_baseline = True: This parameter controls whether the new calculated baseline will be registered in the Model Registry.\n\n\nfrom sagemaker.workflow.quality_check_step import ModelQualityCheckConfig\n\nmodel_quality_baseline_step = QualityCheckStep(\n    name=\"generate-model-quality-baseline\",\n    check_job_config=CheckJobConfig(\n        instance_type=\"ml.c5.xlarge\",\n        instance_count=1,\n        volume_size_in_gb=20,\n        sagemaker_session=config[\"session\"],\n        role=role,\n    ),\n    quality_check_config=ModelQualityCheckConfig(\n        # We are going to use the output of the Transform Step to generate\n        # the model quality baseline.\n        baseline_dataset=generate_test_predictions_step.properties.TransformOutput.S3OutputPath,\n        dataset_format=DatasetFormat.csv(header=False),\n        # We need to specify the problem type and the fields where the prediction\n        # and groundtruth are so the process knows how to interpret the results.\n        problem_type=\"MulticlassClassification\",\n        # Since the data doesn't have headers, SageMaker will autocreate headers for it.\n        # _c0 corresponds to the first column, and _c1 corresponds to the second column.\n        ground_truth_attribute=\"_c0\",\n        inference_attribute=\"_c1\",\n        output_s3_uri=MODEL_QUALITY_LOCATION,\n    ),\n    model_package_group_name=PIPELINE_MODEL_PACKAGE_GROUP,\n    skip_check=True,\n    register_new_baseline=True,\n    cache_config=cache_config,\n)\n\n\n\nStep 5 - Setting up Model Metrics\nWe can configure a new set of ModelMetrics using the results of the Quality Step. Check Baseline and model version lifecycle and evolution with SageMaker Pipelines for an explanation of how SageMaker uses the DriftCheckBaselines.\n\nfrom sagemaker.drift_check_baselines import DriftCheckBaselines\n\nmodel_quality_model_metrics = ModelMetrics(\n    model_statistics=MetricsSource(\n        s3_uri=model_quality_baseline_step.properties.CalculatedBaselineStatistics,\n        content_type=\"application/json\",\n    ),\n    model_constraints=MetricsSource(\n        s3_uri=model_quality_baseline_step.properties.CalculatedBaselineConstraints,\n        content_type=\"application/json\",\n    ),\n    model_data_statistics=MetricsSource(\n        s3_uri=data_quality_baseline_step.properties.CalculatedBaselineStatistics,\n        content_type=\"application/json\",\n    ),\n    model_data_constraints=MetricsSource(\n        s3_uri=data_quality_baseline_step.properties.CalculatedBaselineConstraints,\n        content_type=\"application/json\",\n    ),\n)\n\nmodel_quality_drift_check_baselines = DriftCheckBaselines(\n    model_statistics=MetricsSource(\n        s3_uri=model_quality_baseline_step.properties.BaselineUsedForDriftCheckStatistics,\n        content_type=\"application/json\",\n    ),\n    model_constraints=MetricsSource(\n        s3_uri=model_quality_baseline_step.properties.BaselineUsedForDriftCheckConstraints,\n        content_type=\"application/json\",\n    ),\n    model_data_statistics=MetricsSource(\n        s3_uri=data_quality_baseline_step.properties.BaselineUsedForDriftCheckStatistics,\n        content_type=\"application/json\",\n    ),\n    model_data_constraints=MetricsSource(\n        s3_uri=data_quality_baseline_step.properties.BaselineUsedForDriftCheckConstraints,\n        content_type=\"application/json\",\n    ),\n)\n\n\n\nStep 6 - Registering the Model\nLet’s modify the registration step to use the new metrics and the drift baseline:\n\nregister_model_step = create_registration_step(\n    pipeline_model,\n    PIPELINE_MODEL_PACKAGE_GROUP,\n    content_types=[\"text/csv\", \"application/json\"],\n    response_types=[\"text/csv\", \"application/json\"],\n    model_metrics=model_quality_model_metrics,\n    drift_check_baselines=model_quality_drift_check_baselines,\n)\n\n\n\nStep 7 - Modifying the Condition Step\nWe need to modify the Condition Step to include the new Registration Step and the Transform and Quality Check Steps.\n\ncondition_step = ConditionStep(\n    name=\"check-model-accuracy\",\n    conditions=[condition],\n    if_steps=(\n        [\n            create_model_step,\n            generate_test_predictions_step,\n            model_quality_baseline_step,\n            register_model_step,\n        ]\n    ),\n    else_steps=[fail_step],\n)\n\n\n\nStep 8 - Creating the Pipeline\nWe can now define the SageMaker Pipeline and submit its definition to the SageMaker Pipelines service to create the pipeline if it doesn’t exist or update it if it does.\n\nsession17_pipeline = Pipeline(\n    name=\"session17-pipeline\",\n    parameters=[dataset_location, accuracy_threshold],\n    steps=[\n        preprocessing_step,\n        train_model_step,\n        evaluate_model_step,\n        data_quality_baseline_step,\n        condition_step,\n    ],\n    pipeline_definition_config=pipeline_definition_config,\n    sagemaker_session=config[\"session\"],\n)\n\nsession17_pipeline.upsert(role_arn=role)\n\n\n\nStep 9 - Checking Constraints\nOur pipeline generated model baseline constraints. We can take a look at what these values look like by downloading them from S3. You need to wait for the pipeline to finish running before the file is available.\n\ntry:\n    response = json.loads(\n        S3Downloader.read_file(f\"{MODEL_QUALITY_LOCATION}/constraints.json\"),\n    )\n    print(json.dumps(response, indent=2))\nexcept Exception:  # noqa: S110\n    pass\n\n{\n  \"version\": 0.0,\n  \"multiclass_classification_constraints\": {\n    \"accuracy\": {\n      \"threshold\": 1.0,\n      \"comparison_operator\": \"LessThanThreshold\"\n    },\n    \"weighted_recall\": {\n      \"threshold\": 1.0,\n      \"comparison_operator\": \"LessThanThreshold\"\n    },\n    \"weighted_precision\": {\n      \"threshold\": 1.0,\n      \"comparison_operator\": \"LessThanThreshold\"\n    },\n    \"weighted_f0_5\": {\n      \"threshold\": 1.0,\n      \"comparison_operator\": \"LessThanThreshold\"\n    },\n    \"weighted_f1\": {\n      \"threshold\": 1.0,\n      \"comparison_operator\": \"LessThanThreshold\"\n    },\n    \"weighted_f2\": {\n      \"threshold\": 1.0,\n      \"comparison_operator\": \"LessThanThreshold\"\n    }\n  }\n}",
    "crumbs": [
      "Building Machine Learning Systems That Don't Suck"
    ]
  },
  {
    "objectID": "cohort.html#session-18---data-monitoring",
    "href": "cohort.html#session-18---data-monitoring",
    "title": "Building Machine Learning Systems That Don’t Suck",
    "section": "Session 18 - Data Monitoring",
    "text": "Session 18 - Data Monitoring\nThis session creates a Monitoring Job to monitor the quality of the data received by the endpoint. This schedule will run periodically and check the data that goes into the endpoint against the baseline we generated before.\nCheck Amazon SageMaker Model Monitor for an explanation of how to use SageMaker’s Model Monitoring functionality. Monitor models for data and model quality, bias, and explainability is a much more extensive guide to monitoring in Amazon SageMaker.\n\nStep 1 - Deploying the Model\nLet’s deploy the latest approved model to an endpoint.\nSince we need to do the same later, we can create a function to deploy the model. Notice how we need to enable Data Capture to monitor the data that goes in and out of the endpoint.\n\nfrom sagemaker.model_monitor import DataCaptureConfig\n\n\ndef deploy_model():\n    \"\"\"Deploy the latest model registered in the Model Registry.\"\"\"\n    response = sagemaker_client.list_model_packages(\n        ModelPackageGroupName=PIPELINE_MODEL_PACKAGE_GROUP,\n        ModelApprovalStatus=\"Approved\",\n        SortBy=\"CreationTime\",\n        MaxResults=1,\n    )\n\n    package = (\n        response[\"ModelPackageSummaryList\"][0]\n        if response[\"ModelPackageSummaryList\"]\n        else None\n    )\n\n    if package:\n        model_package = ModelPackage(\n            model_package_arn=package[\"ModelPackageArn\"],\n            sagemaker_session=sagemaker_session,\n            role=role,\n        )\n\n        model_package.deploy(\n            endpoint_name=ENDPOINT,\n            initial_instance_count=1,\n            instance_type=config[\"instance_type\"],\n            # We must enable Data Capture to monitor the model.\n            data_capture_config=DataCaptureConfig(\n                enable_capture=True,\n                sampling_percentage=100,\n                destination_s3_uri=DATA_CAPTURE_DESTINATION,\n                capture_options=[\"REQUEST\", \"RESPONSE\"],\n                csv_content_types=[\"text/csv\"],\n                json_content_types=[\"application/json\"],\n            ),\n        )\n\n\ndeploy_model()\n\n\n\nStep 2 - Generating Fake Traffic\nTo test the monitoring functionality, we need to generate traffic to the endpoint. To generate traffic, we will send every sample from the dataset to the endpoint to simulate real prediction requests:\n\nfrom sagemaker.serializers import JSONSerializer\n\n\ndef generate_fake_traffic():\n    \"\"\"Generate fake traffic to the endpoint.\"\"\"\n    try:\n        for index, row in data.iterrows():\n            payload = \",\".join([str(x) for x in row.to_list()])\n            predictor.predict(\n                payload,\n                initial_args={\"ContentType\": \"text/csv\", \"Accept\": \"text/csv\"},\n                # The `inference_id` field is important to match\n                # it later with a corresponding ground-truth label.\n                inference_id=str(index),\n            )\n    except Exception as e:\n        print(e)\n\n\ngenerate_fake_traffic()\n\nWe can check the location where the endpoint stores the captured data, download a file, and display its content. It may take a few minutes for the first few files to show up in S3.\nThese files contain the data captured by the endpoint in a SageMaker-specific JSON-line format. Each inference request is captured in a single line in the jsonl file. The line contains both the input and output merged together:\n\nfiles = S3Downloader.list(DATA_CAPTURE_DESTINATION)\nif len(files):\n    lines = S3Downloader.read_file(files[-1])\n    print(f\"File: {files[-1]}\")\n    print(json.dumps(json.loads(lines.split(\"\\n\")[0]), indent=2))\n\nFile: s3://mlschool/penguins/monitoring/data-capture/penguins-endpoint/AllTraffic/2024/03/30/17/32-02-242-191b135d-085a-484d-a119-45b26c51554c.jsonl\n{\n  \"captureData\": {\n    \"endpointInput\": {\n      \"observedContentType\": \"text/csv\",\n      \"mode\": \"INPUT\",\n      \"data\": \"Torgersen,39.1,18.7,181.0,3750.0,MALE\",\n      \"encoding\": \"CSV\"\n    },\n    \"endpointOutput\": {\n      \"observedContentType\": \"text/csv; charset=utf-8\",\n      \"mode\": \"OUTPUT\",\n      \"data\": \"Adelie,0.964408875\\n\",\n      \"encoding\": \"CSV\"\n    }\n  },\n  \"eventMetadata\": {\n    \"eventId\": \"3211434d-0db6-4ee2-8848-95ce11f6d5e6\",\n    \"inferenceId\": \"0\",\n    \"inferenceTime\": \"2024-03-30T17:32:02Z\"\n  },\n  \"eventVersion\": \"0\"\n}\n\n\n\n\nStep 3 - Creating Custom Preprocessing Script\nSageMaker looks for violations in the data captured by the endpoint. By default, it combines the input data with the endpoint output and compares the result with the baseline we generated before. If we let SageMaker do this, we will get a few violations, for example an “extra column check” violation because the field confidence doesn’t exist in the baseline data.\nWe can fix these violations by creating a preprocessing script configuring the data we want the monitoring job to use. Check Preprocessing and Postprocessing for more information about how to configure these scripts.\nWe’ll store the script in a folder called monitoring:\n\nDATA_QUALITY_PREPROCESSOR = \"data_quality_preprocessor.py\"\n\n(CODE_FOLDER / \"monitoring\").mkdir(parents=True, exist_ok=True)\n\nWe can now define the preprocessing script. Notice that this script will return a JSON object with a name for each feature and their value.\n\n\n\ndata_quality_preprocessor.py\n\nimport json\n\n\ndef preprocess_handler(inference_record, logger):\n    input_data = inference_record.endpoint_input.data\n    return {str(i).zfill(2): d for i, d in enumerate(input_data.split(\",\"))}\n\n\n\n\nStep 4 - Uploading Preprocessing Script\nThe monitoring schedule expects an S3 location pointing to the preprocessing script. Let’s upload the script to the default bucket.\n\nbucket = boto3.Session().resource(\"s3\").Bucket(config[\"session\"].default_bucket())\nprefix = Path(\"penguins/monitoring\")\nbucket.Object((prefix / DATA_QUALITY_PREPROCESSOR).as_posix()).upload_file(\n    (CODE_FOLDER / \"monitoring\" / DATA_QUALITY_PREPROCESSOR).as_posix(),\n)\ndata_quality_preprocessor = f\"s3://{(bucket.name / prefix / DATA_QUALITY_PREPROCESSOR)}\"\ndata_quality_preprocessor\n\n\n\nStep 5 - Creating Monitoring Schedule\nWe can now set up the Data Quality Monitoring Job using the DefaultModelMonitor class.\n\nfrom sagemaker.model_monitor import DefaultModelMonitor\n\ndata_monitor = DefaultModelMonitor(\n    instance_type=config[\"instance_type\"],\n    instance_count=1,\n    max_runtime_in_seconds=1800,\n    volume_size_in_gb=20,\n    role=role,\n)\n\nINFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: .\nINFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n\n\nLet’s now create the monitoring schedule. Notice how we specify the record_preprocessor_script using the S3 location where we uploaded our script.\nWe are going to set up the monitoring schedule to run every hour. Keep in mind that SageMaker has a buffer period of 20 minutes to schedule an execution.\n\nimport time\nfrom sagemaker.model_monitor import CronExpressionGenerator\n\ndata_monitor.create_monitoring_schedule(\n    monitor_schedule_name=\"penguins-data-monitoring-schedule\",\n    endpoint_input=ENDPOINT,\n    record_preprocessor_script=data_quality_preprocessor,\n    statistics=f\"{DATA_QUALITY_LOCATION}/statistics.json\",\n    constraints=f\"{DATA_QUALITY_LOCATION}/constraints.json\",\n    schedule_cron_expression=CronExpressionGenerator.hourly(),\n    output_s3_uri=DATA_QUALITY_LOCATION,\n    enable_cloudwatch_metrics=True,\n)\n\n# Let's give SageMaker some time to process the\n# monitoring job before we start it.\ntime.sleep(10)\ndata_monitor.start_monitoring_schedule()\n\n\n\nStep 6 - Checking Violations\nAfter the monitoring schedule runs for the first time, we can check the results of the last execution. If the job completed successfully, we can check if there are any violations.\n\ndef check_execution(monitoring_schedule):\n    \"\"\"Check the execution of the Monitoring Job.\n\n    This function checks the execution of the Monitoring\n    Job and prints out the list of violations if the job\n    completed.\n    \"\"\"\n    try:\n        executions = monitoring_schedule.list_executions()\n\n        if executions:\n            execution = executions[-1].describe()\n            print(f\"Processing Job Status: {execution['ProcessingJobStatus']}\")\n\n            if execution[\"ProcessingJobStatus\"] == \"Completed\":\n                print(f\"Exit Message: \\\"{execution['ExitMessage']}\\\"\")\n                print(\n                    f\"Last Modified Time: {execution['LastModifiedTime']}\",\n                    end=\"\\n\\n\",\n                )\n                print(\"Execution:\")\n                print(json.dumps(execution, default=str, indent=2), end=\"\\n\\n\")\n\n                latest_monitoring_violations = (\n                    monitoring_schedule.latest_monitoring_constraint_violations()\n                )\n                response = json.loads(\n                    S3Downloader.read_file(latest_monitoring_violations.file_s3_uri),\n                )\n                print(\"Violations:\")\n                print(json.dumps(response, indent=2))\n    except Exception as e:\n        print(e)\n\n\ncheck_execution(data_monitor)\n\nProcessing Job Status: Completed\nExit Message: \"Completed: Job completed successfully with no violations.\"\nLast Modified Time: 2024-03-30 14:15:49.146000-04:00\n\nExecution:\n{\n  \"ProcessingInputs\": [\n    {\n      \"InputName\": \"baseline\",\n      \"AppManaged\": false,\n      \"S3Input\": {\n        \"S3Uri\": \"s3://mlschool/penguins/monitoring/data-quality/statistics.json\",\n        \"LocalPath\": \"/opt/ml/processing/baseline/stats\",\n        \"S3DataType\": \"S3Prefix\",\n        \"S3InputMode\": \"File\",\n        \"S3DataDistributionType\": \"FullyReplicated\"\n      }\n    },\n    {\n      \"InputName\": \"constraints\",\n      \"AppManaged\": false,\n      \"S3Input\": {\n        \"S3Uri\": \"s3://mlschool/penguins/monitoring/data-quality/constraints.json\",\n        \"LocalPath\": \"/opt/ml/processing/baseline/constraints\",\n        \"S3DataType\": \"S3Prefix\",\n        \"S3InputMode\": \"File\",\n        \"S3DataDistributionType\": \"FullyReplicated\"\n      }\n    },\n    {\n      \"InputName\": \"pre_processor_script\",\n      \"AppManaged\": false,\n      \"S3Input\": {\n        \"S3Uri\": \"s3://mlschool/penguins/monitoring/data_quality_preprocessor.py\",\n        \"LocalPath\": \"/opt/ml/processing/code/preprocessing\",\n        \"S3DataType\": \"S3Prefix\",\n        \"S3InputMode\": \"File\",\n        \"S3DataDistributionType\": \"FullyReplicated\"\n      }\n    },\n    {\n      \"InputName\": \"endpoint_input_1\",\n      \"AppManaged\": false,\n      \"S3Input\": {\n        \"S3Uri\": \"s3://mlschool/penguins/monitoring/data-capture/penguins-endpoint/AllTraffic/2024/03/30/17\",\n        \"LocalPath\": \"/opt/ml/processing/input/endpoint/penguins-endpoint/AllTraffic/2024/03/30/17\",\n        \"S3DataType\": \"S3Prefix\",\n        \"S3InputMode\": \"File\",\n        \"S3DataDistributionType\": \"FullyReplicated\",\n        \"S3CompressionType\": \"None\"\n      }\n    }\n  ],\n  \"ProcessingOutputConfig\": {\n    \"Outputs\": [\n      {\n        \"OutputName\": \"result\",\n        \"S3Output\": {\n          \"S3Uri\": \"s3://mlschool/penguins/monitoring/data-quality/penguins-endpoint/penguins-data-monitoring-schedule/2024/03/30/18\",\n          \"LocalPath\": \"/opt/ml/processing/output\",\n          \"S3UploadMode\": \"Continuous\"\n        },\n        \"AppManaged\": false\n      }\n    ]\n  },\n  \"ProcessingJobName\": \"model-monitoring-202403301800-17aa1fca873fac795ffba24a\",\n  \"ProcessingResources\": {\n    \"ClusterConfig\": {\n      \"InstanceCount\": 1,\n      \"InstanceType\": \"ml.m5.xlarge\",\n      \"VolumeSizeInGB\": 20\n    }\n  },\n  \"StoppingCondition\": {\n    \"MaxRuntimeInSeconds\": 1800\n  },\n  \"AppSpecification\": {\n    \"ImageUri\": \"156813124566.dkr.ecr.us-east-1.amazonaws.com/sagemaker-model-monitor-analyzer\"\n  },\n  \"Environment\": {\n    \"baseline_constraints\": \"/opt/ml/processing/baseline/constraints/constraints.json\",\n    \"baseline_statistics\": \"/opt/ml/processing/baseline/stats/statistics.json\",\n    \"dataset_format\": \"{\\\"sagemakerCaptureJson\\\":{\\\"captureIndexNames\\\":[\\\"endpointInput\\\",\\\"endpointOutput\\\"]}}\",\n    \"dataset_source\": \"/opt/ml/processing/input/endpoint\",\n    \"end_time\": \"2024-03-30T18:00:00Z\",\n    \"metric_time\": \"2024-03-30T17:00:00Z\",\n    \"monitoring_input_type\": \"ENDPOINT_INPUT\",\n    \"output_path\": \"/opt/ml/processing/output\",\n    \"publish_cloudwatch_metrics\": \"Enabled\",\n    \"record_preprocessor_script\": \"/opt/ml/processing/code/preprocessing/data_quality_preprocessor.py\",\n    \"sagemaker_endpoint_name\": \"penguins-endpoint\",\n    \"sagemaker_monitoring_schedule_name\": \"penguins-data-monitoring-schedule\",\n    \"start_time\": \"2024-03-30T17:00:00Z\"\n  },\n  \"RoleArn\": \"arn:aws:iam::325223348818:role/service-role/AmazonSageMaker-ExecutionRole-20230312T160501\",\n  \"ProcessingJobArn\": \"arn:aws:sagemaker:us-east-1:325223348818:processing-job/model-monitoring-202403301800-17aa1fca873fac795ffba24a\",\n  \"ProcessingJobStatus\": \"Completed\",\n  \"ExitMessage\": \"Completed: Job completed successfully with no violations.\",\n  \"ProcessingEndTime\": \"2024-03-30 14:15:48.732000-04:00\",\n  \"ProcessingStartTime\": \"2024-03-30 14:14:14.760000-04:00\",\n  \"LastModifiedTime\": \"2024-03-30 14:15:49.146000-04:00\",\n  \"CreationTime\": \"2024-03-30 14:09:54.896000-04:00\",\n  \"MonitoringScheduleArn\": \"arn:aws:sagemaker:us-east-1:325223348818:monitoring-schedule/penguins-data-monitoring-schedule\",\n  \"ResponseMetadata\": {\n    \"RequestId\": \"4e348652-7dff-4c40-96fb-b944aa6ed83b\",\n    \"HTTPStatusCode\": 200,\n    \"HTTPHeaders\": {\n      \"x-amzn-requestid\": \"4e348652-7dff-4c40-96fb-b944aa6ed83b\",\n      \"content-type\": \"application/x-amz-json-1.1\",\n      \"content-length\": \"3233\",\n      \"date\": \"Sat, 30 Mar 2024 18:34:16 GMT\"\n    },\n    \"RetryAttempts\": 0\n  }\n}\n\nViolations:\n{\n  \"violations\": []\n}\n\n\n\n\nStep 7 - Deleting Monitoring Schedule\nOnce we are done with it, we can delete the Data Monitoring schedule.\n\ntry:\n    data_monitor.delete_monitoring_schedule()\nexcept Exception as e:\n    print(e)",
    "crumbs": [
      "Building Machine Learning Systems That Don't Suck"
    ]
  },
  {
    "objectID": "cohort.html#session-19---model-monitoring",
    "href": "cohort.html#session-19---model-monitoring",
    "title": "Building Machine Learning Systems That Don’t Suck",
    "section": "Session 19 - Model Monitoring",
    "text": "Session 19 - Model Monitoring\nThis session creates a Monitoring Job to monitor the quality of the model outputs. This schedule will run periodically and check the data that goes into the endpoint against the baseline we generated before.\nCheck Amazon SageMaker Model Monitor for an explanation of how to use SageMaker’s Model Monitoring functionality. Monitor models for data and model quality, bias, and explainability is a much more extensive guide to monitoring in Amazon SageMaker.\n\nStep 1 - Configuring Ground Truth Location\nLet’s start by defining the location where SageMaker will store the ground-truth generated by labeling the data received by the endpoint.\n\nGROUND_TRUTH_LOCATION = f\"{S3_LOCATION}/monitoring/groundtruth\"\n\n\n\nStep 2 - Deploying the Model\nLet’s deploy the latest approved model to an endpoint.\nHere, we can reuse the function we created before to deploy the model.\n\ndeploy_model()\n\n\n\nStep 3 - Generating Fake Traffic\nTo test the monitoring functionality, we need to generate traffic to the endpoint. We can use the function we created before to generate fake traffic to the endpoint.\n\ngenerate_fake_traffic()\n\nWe can check the location where the endpoint stores the captured data, download a file, and display its content. It may take a few minutes for the first few files to show up in S3.\nThese files contain the data captured by the endpoint in a SageMaker-specific JSON-line format. Each inference request is captured in a single line in the jsonl file. The line contains both the input and output merged together:\n\nfiles = S3Downloader.list(DATA_CAPTURE_DESTINATION)\nif len(files):\n    lines = S3Downloader.read_file(files[-1])\n    print(f\"File: {files[-1]}\")\n    print(json.dumps(json.loads(lines.split(\"\\n\")[0]), indent=2))\n\nFile: s3://mlschool/penguins/monitoring/data-capture/penguins-endpoint/AllTraffic/2024/03/30/18/40-45-068-0f144be9-ac73-4c4e-a0c7-82b1ba7db88b.jsonl\n{\n  \"captureData\": {\n    \"endpointInput\": {\n      \"observedContentType\": \"text/csv\",\n      \"mode\": \"INPUT\",\n      \"data\": \"Torgersen,39.1,18.7,181.0,3750.0,MALE\",\n      \"encoding\": \"CSV\"\n    },\n    \"endpointOutput\": {\n      \"observedContentType\": \"text/csv; charset=utf-8\",\n      \"mode\": \"OUTPUT\",\n      \"data\": \"Adelie,0.964408875\\n\",\n      \"encoding\": \"CSV\"\n    }\n  },\n  \"eventMetadata\": {\n    \"eventId\": \"08a239af-c98c-4984-b9bf-4ea049d88617\",\n    \"inferenceId\": \"0\",\n    \"inferenceTime\": \"2024-03-30T18:40:45Z\"\n  },\n  \"eventVersion\": \"0\"\n}\n\n\n\n\nStep 4 - Generating Fake Labels\nTo test the performance of the model, we need to label the samples captured by the endpoint. We can simulate the labeling process by generating a random label for every sample. Check Ingest Ground Truth Labels and Merge Them With Predictions for more information about this.\n\nimport random\nfrom datetime import datetime, timezone\n\nfrom sagemaker.s3 import S3Uploader\n\nrecords = []\nfor inference_id in range(len(data)):\n    random.seed(inference_id)\n\n    records.append(\n        json.dumps(\n            {\n                \"groundTruthData\": {\n                    # For testing purposes, we will generate a random\n                    # label for each request.\n                    \"data\": random.choice([\"Adelie\", \"Chinstrap\", \"Gentoo\"]),\n                    \"encoding\": \"CSV\",\n                },\n                \"eventMetadata\": {\n                    # This value should match the id of the request\n                    # captured by the endpoint.\n                    \"eventId\": str(inference_id),\n                },\n                \"eventVersion\": \"0\",\n            },\n        ),\n    )\n\ngroundtruth_payload = \"\\n\".join(records)\nupload_time = datetime.now(tz=timezone.utc)\nuri = f\"{GROUND_TRUTH_LOCATION}/{upload_time:%Y/%m/%d/%H/%M%S}.jsonl\"\nS3Uploader.upload_string_as_file_body(groundtruth_payload, uri)\n\n\n\nStep 5 - Creating Monitoring Schedule\nTo set up a Model Quality Monitoring Job, we can use the ModelQualityMonitor class.\nCheck Amazon SageMaker Model Quality Monitor for a complete tutorial on how to run a Model Monitoring Job in SageMaker.\n\nfrom sagemaker.model_monitor import ModelQualityMonitor\n\nmodel_monitor = ModelQualityMonitor(\n    instance_type=config[\"instance_type\"],\n    instance_count=1,\n    max_runtime_in_seconds=1800,\n    volume_size_in_gb=20,\n    role=role,\n)\n\nLet’s now create the monitoring schedule. The EndpointInput instance configures the attribute the monitoring job should use to determine the prediction from the model.\nWe are going to set up the monitoring schedule to run every hour. Keep in mind that SageMaker has a buffer period of 20 minutes to schedule an execution.\n\nimport time\n\nfrom sagemaker.model_monitor import CronExpressionGenerator, EndpointInput\n\nmodel_monitor.create_monitoring_schedule(\n    monitor_schedule_name=\"penguins-model-monitoring-schedule\",\n    endpoint_input=EndpointInput(\n        endpoint_name=ENDPOINT,\n        # The first attribute is the prediction made\n        # by the model. For example, here is a\n        # potential output from the model:\n        # [Adelie,0.977324724\\n]\n        inference_attribute=\"0\",\n        destination=\"/opt/ml/processing/input_data\",\n    ),\n    problem_type=\"MulticlassClassification\",\n    ground_truth_input=GROUND_TRUTH_LOCATION,\n    constraints=f\"{MODEL_QUALITY_LOCATION}/constraints.json\",\n    schedule_cron_expression=CronExpressionGenerator.hourly(),\n    output_s3_uri=MODEL_QUALITY_LOCATION,\n    enable_cloudwatch_metrics=True,\n)\n\n# Let's give SageMaker some time to process the\n# monitoring job before we start it.\ntime.sleep(10)\nmodel_monitor.start_monitoring_schedule()\n\n\n\nStep 6 - Checking Violations\nAfter the monitoring schedule runs for the first time, we can check the results of the last execution. If the job completed successfully, we can check if there are any violations.\n\ncheck_execution(model_monitor)\n\nProcessing Job Status: Completed\nExit Message: \"CompletedWithViolations: Job completed successfully with 5 violations.\"\nLast Modified Time: 2024-03-30 15:18:36.431000-04:00\n\nExecution:\n{\n  \"ProcessingInputs\": [\n    {\n      \"InputName\": \"constraints\",\n      \"AppManaged\": false,\n      \"S3Input\": {\n        \"S3Uri\": \"s3://mlschool/penguins/monitoring/model-quality/constraints.json\",\n        \"LocalPath\": \"/opt/ml/processing/baseline/constraints\",\n        \"S3DataType\": \"S3Prefix\",\n        \"S3InputMode\": \"File\",\n        \"S3DataDistributionType\": \"FullyReplicated\"\n      }\n    },\n    {\n      \"InputName\": \"endpoint_input_1\",\n      \"AppManaged\": false,\n      \"S3Input\": {\n        \"S3Uri\": \"s3://mlschool/penguins/monitoring/model-quality/merge/penguins-endpoint/AllTraffic/2024/03/30/18\",\n        \"LocalPath\": \"/opt/ml/processing/input_data/penguins-endpoint/AllTraffic/2024/03/30/18\",\n        \"S3DataType\": \"S3Prefix\",\n        \"S3InputMode\": \"File\",\n        \"S3DataDistributionType\": \"FullyReplicated\",\n        \"S3CompressionType\": \"None\"\n      }\n    }\n  ],\n  \"ProcessingOutputConfig\": {\n    \"Outputs\": [\n      {\n        \"OutputName\": \"result\",\n        \"S3Output\": {\n          \"S3Uri\": \"s3://mlschool/penguins/monitoring/model-quality/penguins-endpoint/penguins-model-monitoring-schedule/2024/03/30/19\",\n          \"LocalPath\": \"/opt/ml/processing/output\",\n          \"S3UploadMode\": \"Continuous\"\n        },\n        \"AppManaged\": false\n      }\n    ]\n  },\n  \"ProcessingJobName\": \"model-quality-monitoring-202403301900-896e874cc3a809cdf37d6cc2\",\n  \"ProcessingResources\": {\n    \"ClusterConfig\": {\n      \"InstanceCount\": 1,\n      \"InstanceType\": \"ml.m5.xlarge\",\n      \"VolumeSizeInGB\": 20\n    }\n  },\n  \"StoppingCondition\": {\n    \"MaxRuntimeInSeconds\": 1800\n  },\n  \"AppSpecification\": {\n    \"ImageUri\": \"156813124566.dkr.ecr.us-east-1.amazonaws.com/sagemaker-model-monitor-analyzer\"\n  },\n  \"Environment\": {\n    \"analysis_type\": \"MODEL_QUALITY\",\n    \"baseline_constraints\": \"/opt/ml/processing/baseline/constraints/constraints.json\",\n    \"dataset_format\": \"{\\\"sagemakerMergeJson\\\":{\\\"captureIndexNames\\\":[\\\"endpointOutput\\\"],\\\"originalDatasetFormat\\\":null}}\",\n    \"dataset_source\": \"/opt/ml/processing/input_data\",\n    \"end_time\": \"2024-03-30T19:00:00Z\",\n    \"inference_attribute\": \"0\",\n    \"metric_time\": \"2024-03-30T18:00:00Z\",\n    \"monitoring_input_type\": \"ENDPOINT_INPUT\",\n    \"output_path\": \"/opt/ml/processing/output\",\n    \"problem_type\": \"MulticlassClassification\",\n    \"publish_cloudwatch_metrics\": \"Enabled\",\n    \"sagemaker_endpoint_name\": \"penguins-endpoint\",\n    \"sagemaker_monitoring_schedule_name\": \"penguins-model-monitoring-schedule\",\n    \"start_time\": \"2024-03-30T18:00:00Z\"\n  },\n  \"RoleArn\": \"arn:aws:iam::325223348818:role/service-role/AmazonSageMaker-ExecutionRole-20230312T160501\",\n  \"ProcessingJobArn\": \"arn:aws:sagemaker:us-east-1:325223348818:processing-job/model-quality-monitoring-202403301900-896e874cc3a809cdf37d6cc2\",\n  \"ProcessingJobStatus\": \"Completed\",\n  \"ExitMessage\": \"CompletedWithViolations: Job completed successfully with 5 violations.\",\n  \"ProcessingEndTime\": \"2024-03-30 15:18:35.908000-04:00\",\n  \"ProcessingStartTime\": \"2024-03-30 15:16:52.922000-04:00\",\n  \"LastModifiedTime\": \"2024-03-30 15:18:36.431000-04:00\",\n  \"CreationTime\": \"2024-03-30 15:12:22.569000-04:00\",\n  \"MonitoringScheduleArn\": \"arn:aws:sagemaker:us-east-1:325223348818:monitoring-schedule/penguins-model-monitoring-schedule\",\n  \"ResponseMetadata\": {\n    \"RequestId\": \"85abb737-543a-4c92-928b-4a293c599f18\",\n    \"HTTPStatusCode\": 200,\n    \"HTTPHeaders\": {\n      \"x-amzn-requestid\": \"85abb737-543a-4c92-928b-4a293c599f18\",\n      \"content-type\": \"application/x-amz-json-1.1\",\n      \"content-length\": \"2660\",\n      \"date\": \"Sat, 30 Mar 2024 19:33:23 GMT\"\n    },\n    \"RetryAttempts\": 0\n  }\n}\n\nViolations:\n{\n  \"violations\": [\n    {\n      \"constraint_check_type\": \"LessThanThreshold\",\n      \"description\": \"Metric weightedF2 with 0.3518870011147463 +/- 0.006730551075118943 was LessThanThreshold '1.0'\",\n      \"metric_name\": \"weightedF2\"\n    },\n    {\n      \"constraint_check_type\": \"LessThanThreshold\",\n      \"description\": \"Metric accuracy with 0.35755813953488375 +/- 0.007228798319401767 was LessThanThreshold '1.0'\",\n      \"metric_name\": \"accuracy\"\n    },\n    {\n      \"constraint_check_type\": \"LessThanThreshold\",\n      \"description\": \"Metric weightedRecall with 0.35755813953488375 +/- 0.007228798319401765 was LessThanThreshold '1.0'\",\n      \"metric_name\": \"weightedRecall\"\n    },\n    {\n      \"constraint_check_type\": \"LessThanThreshold\",\n      \"description\": \"Metric weightedPrecision with 0.35624627310673823 +/- 0.008910206698382583 was LessThanThreshold '1.0'\",\n      \"metric_name\": \"weightedPrecision\"\n    },\n    {\n      \"constraint_check_type\": \"LessThanThreshold\",\n      \"description\": \"Metric weightedF1 with 0.34769539574160063 +/- 0.006655863903356062 was LessThanThreshold '1.0'\",\n      \"metric_name\": \"weightedF1\"\n    }\n  ]\n}\n\n\n\n\nStep 7 - Deleting Monitoring Schedule\nOnce we are done with it, we can delete the Data Monitoring schedule.\n\ntry:\n    model_monitor.delete_monitoring_schedule()\nexcept Exception as e:\n    print(e)",
    "crumbs": [
      "Building Machine Learning Systems That Don't Suck"
    ]
  },
  {
    "objectID": "cohort.html#session-20---shadow-deployments",
    "href": "cohort.html#session-20---shadow-deployments",
    "title": "Building Machine Learning Systems That Don’t Suck",
    "section": "Session 20 - Shadow Deployments",
    "text": "Session 20 - Shadow Deployments\nThis session configures an endpoint running a production and a shadow variant. Check Safely validate models in production for more information.\n \n\nStep 1 - Getting The Latest Models\nWe want to deploy the two latest approved models from the Model Registry to the same endpoint. The latest version of the model will act as the Shadow variant, and the previous version will act as the Production variant.\n\nresponse = sagemaker_client.list_model_packages(\n    ModelPackageGroupName=BASIC_MODEL_PACKAGE_GROUP,\n    ModelApprovalStatus=\"Approved\",\n    SortBy=\"CreationTime\",\n    MaxResults=2,\n)\n\nif response[\"ModelPackageSummaryList\"]:\n    production_package = response[\"ModelPackageSummaryList\"][1][\"ModelPackageArn\"]\n    shadow_package = response[\"ModelPackageSummaryList\"][0][\"ModelPackageArn\"]\nelse:\n    production_package = None\n    shadow_package = None\n\nprint(f\"Production package: {production_package}\")\nprint(f\"Shadow package: {shadow_package}\")\n\nProduction package: arn:aws:sagemaker:us-east-1:325223348818:model-package/basic-penguins/5\nShadow package: arn:aws:sagemaker:us-east-1:325223348818:model-package/basic-penguins/6\n\n\n\n\nStep 2 - Creating the Models\nWe want to deploy the two packages to a new endpoint. We’ll use the boto3 API to deploy these models.\nLet’s start by creating the SageMaker Models.\n\nimport time\n\n# We'll use a different name for this endpoint.\nSHADOW_DEPLOYMENT_ENDPOINT = \"shadow-penguins-endpoint\"\n\n# The timestamp will help us create unique name for the\n# name of the models.\ntimestamp = time.strftime(\"%m%d%H%M%S\", time.localtime())\n\nLet’s now create the Production model.\n\nproduction_model_name = f\"{SHADOW_DEPLOYMENT_ENDPOINT}-production-{timestamp}\"\n\nsagemaker_client.create_model(\n    ModelName=production_model_name,\n    ExecutionRoleArn=role,\n    Containers=[{\"ModelPackageName\": production_package}],\n)\n\nAnd now we can create the second model.\n\nshadow_model_name = f\"{SHADOW_DEPLOYMENT_ENDPOINT}-shadow-{timestamp}\"\n\nsagemaker_client.create_model(\n    ModelName=shadow_model_name,\n    ExecutionRoleArn=role,\n    Containers=[{\"ModelPackageName\": shadow_package}],\n)\n\n{'ModelArn': 'arn:aws:sagemaker:us-east-1:325223348818:model/shadow-penguins-endpoint-shadow-0331125310',\n 'ResponseMetadata': {'RequestId': '21aaeb87-98e5-49c3-8912-1143ef75f86c',\n  'HTTPStatusCode': 200,\n  'HTTPHeaders': {'x-amzn-requestid': '21aaeb87-98e5-49c3-8912-1143ef75f86c',\n   'content-type': 'application/x-amz-json-1.1',\n   'content-length': '104',\n   'date': 'Sun, 31 Mar 2024 16:53:13 GMT'},\n  'RetryAttempts': 0}}\n\n\n\n\nStep 3 - Creating the Endpoint Configuration\nWe can now create the Endpoint Configuration using the two models\nLet’s define the location where SageMaker will output the information captured by the Shadow variant.\n\nSHADOW_DATA_DESTINATION = f\"{S3_LOCATION}/endpoint/\"\n\nWe can create the Endpoint Configuration now.\n\nendpoint_config_name = f\"{SHADOW_DEPLOYMENT_ENDPOINT}-config-{timestamp}\"\n\nsagemaker_client.create_endpoint_config(\n    EndpointConfigName=endpoint_config_name,\n    ProductionVariants=[\n        {\n            \"ModelName\": production_model_name,\n            \"InstanceType\": \"ml.m5.xlarge\",\n            \"InitialVariantWeight\": 1,\n            \"InitialInstanceCount\": 1,\n            \"VariantName\": \"ProductionTraffic\",\n        },\n    ],\n    ShadowProductionVariants=[\n        {\n            \"ModelName\": shadow_model_name,\n            \"InstanceType\": \"ml.m5.xlarge\",\n            \"InitialVariantWeight\": 1,\n            \"InitialInstanceCount\": 1,\n            \"VariantName\": \"ShadowTraffic\",\n        },\n    ],\n    DataCaptureConfig={\n        \"EnableCapture\": True,\n        \"InitialSamplingPercentage\": 100,\n        \"DestinationS3Uri\": SHADOW_DATA_DESTINATION,\n        \"CaptureOptions\": [\n            {\"CaptureMode\": \"Input\"},\n            {\"CaptureMode\": \"Output\"},\n        ],\n        \"CaptureContentTypeHeader\": {\n            \"CsvContentTypes\": [\"text/csv\", \"application/octect-stream\"],\n            \"JsonContentTypes\": [\"application/json\", \"application/octect-stream\"],\n        },\n    },\n)\n\n{'EndpointConfigArn': 'arn:aws:sagemaker:us-east-1:325223348818:endpoint-config/shadow-penguins-endpoint-config-0331125310',\n 'ResponseMetadata': {'RequestId': '24973c88-6726-4737-ae91-1138b77f5775',\n  'HTTPStatusCode': 200,\n  'HTTPHeaders': {'x-amzn-requestid': '24973c88-6726-4737-ae91-1138b77f5775',\n   'content-type': 'application/x-amz-json-1.1',\n   'content-length': '123',\n   'date': 'Sun, 31 Mar 2024 16:53:17 GMT'},\n  'RetryAttempts': 0}}\n\n\n\n\nStep 4 - Creating the Endpoint\nFinally, we can create the Endpoint using the Endpoint Configuration we created before.\n\nsagemaker_client.create_endpoint(\n    EndpointName=SHADOW_DEPLOYMENT_ENDPOINT,\n    EndpointConfigName=endpoint_config_name,\n)\n\n{'EndpointArn': 'arn:aws:sagemaker:us-east-1:325223348818:endpoint/shadow-penguins-endpoint',\n 'ResponseMetadata': {'RequestId': 'df5ebd20-f59f-4895-96f4-18da3beb0cc4',\n  'HTTPStatusCode': 200,\n  'HTTPHeaders': {'x-amzn-requestid': 'df5ebd20-f59f-4895-96f4-18da3beb0cc4',\n   'content-type': 'application/x-amz-json-1.1',\n   'content-length': '92',\n   'date': 'Sun, 31 Mar 2024 16:53:21 GMT'},\n  'RetryAttempts': 0}}\n\n\n\n\nStep 5 - Generating Traffic\nLet’s generate some traffic to the endpoint so we can test the Shadow variant.\n\npayload = \"\"\"\n0.6569590202313976,-1.0813829646495108,1.2097102831892812,0.9226343641317372,1.0,0.0,0.0\n-0.7751048801481084,0.8822689351285553,-1.2168066120762704,0.9226343641317372,0.0,1.0,0.0\n-0.837387834894918,0.3386660813829646,-0.26237731892812,-1.92351941317372,0.0,0.0,1.0\n\"\"\"\n\npredictor = Predictor(\n    endpoint_name=SHADOW_DEPLOYMENT_ENDPOINT,\n    serializer=CSVSerializer(),\n    deserializer=JSONDeserializer(),\n)\n\ntry:\n    response = predictor.predict(payload)\n    print(json.dumps(response, indent=2))\nexcept Exception as e:\n    print(e)\n\n{\n  \"predictions\": [\n    [\n      0.0403208546,\n      0.0210227184,\n      0.93865639\n    ],\n    [\n      0.689678669,\n      0.17514421,\n      0.135177106\n    ],\n    [\n      0.960919619,\n      0.0248175282,\n      0.0142629147\n    ]\n  ]\n}\n\n\n\n\nStep 6 - Checking Captured Data\nLet’s check the location where the endpoint stores the captured data, download a file, and display its content. It may take a few minutes for the first few files to show up in S3.\nThe endpoint will capture the data for both the Production and the Shadow variants.\n\nfiles = S3Downloader.list(\n    f\"{SHADOW_DATA_DESTINATION}{SHADOW_DEPLOYMENT_ENDPOINT}/ShadowTraffic/\",\n)\nif len(files):\n    lines = S3Downloader.read_file(files[-1])\n    print(f\"File: {files[-1]}\")\n    print(json.dumps(json.loads(lines.split(\"\\n\")[0]), indent=2))\n\nFile: s3://mlschool/penguins/endpoint/shadow-penguins-endpoint/ShadowTraffic/2024/03/30/21/28-43-624-8f47e605-6bd2-44dd-bd91-293f29fd227e.jsonl\n{\n  \"captureData\": {\n    \"endpointInput\": {\n      \"observedContentType\": \"text/csv\",\n      \"mode\": \"INPUT\",\n      \"data\": \"\\n0.6569590202313976,-1.0813829646495108,1.2097102831892812,0.9226343641317372,1.0,0.0,0.0\\n-0.7751048801481084,0.8822689351285553,-1.2168066120762704,0.9226343641317372,0.0,1.0,0.0\\n-0.837387834894918,0.3386660813829646,-0.26237731892812,-1.92351941317372,0.0,0.0,1.0\\n\",\n      \"encoding\": \"CSV\"\n    },\n    \"endpointOutput\": {\n      \"observedContentType\": \"application/json\",\n      \"mode\": \"OUTPUT\",\n      \"data\": \"{    \\\"predictions\\\": [[0.124825425, 0.0847824216, 0.79039216], [0.766525269, 0.220783874, 0.0126908608], [0.944253445, 0.0292692278, 0.0264772158]    ]}\",\n      \"encoding\": \"JSON\"\n    }\n  },\n  \"eventMetadata\": {\n    \"eventId\": \"98c3c22e-20af-401c-9ca6-6d67d734a83f\",\n    \"invocationSource\": \"ShadowExperiment\",\n    \"inferenceTime\": \"2024-03-30T21:28:43Z\"\n  },\n  \"eventVersion\": \"0\"\n}\n\n\n\n\nStep 7 - Deleting the Endpoint\nLet’s now delete the endpoint.\n\ntry:\n    sagemaker_client.delete_endpoint(EndpointName=SHADOW_DEPLOYMENT_ENDPOINT)\nexcept Exception as e:\n    print(e)",
    "crumbs": [
      "Building Machine Learning Systems That Don't Suck"
    ]
  },
  {
    "objectID": "cohort.html#running-the-pipeline",
    "href": "cohort.html#running-the-pipeline",
    "title": "Building Machine Learning Systems That Don’t Suck",
    "section": "Running the Pipeline",
    "text": "Running the Pipeline\nWe can run any of the pipelines we defined before by enabling the cell below and specifying the pipeline we want to run.\n\nsession3_pipeline.start()",
    "crumbs": [
      "Building Machine Learning Systems That Don't Suck"
    ]
  },
  {
    "objectID": "cohort.html#deleting-the-endpoint",
    "href": "cohort.html#deleting-the-endpoint",
    "title": "Building Machine Learning Systems That Don’t Suck",
    "section": "Deleting the Endpoint",
    "text": "Deleting the Endpoint\nAfter testing the endpoint, we need to ensure we delete it.\n\ntry:\n    sagemaker_client.delete_endpoint(EndpointName=ENDPOINT)\nexcept Exception as e:\n    print(e)",
    "crumbs": [
      "Building Machine Learning Systems That Don't Suck"
    ]
  }
]